<html lang="en"><head><script>(function(firebaseConfig, initialAuthToken, appId) {
        window.__firebase_config = firebaseConfig;
        window.__initial_auth_token = initialAuthToken;
        window.__app_id = appId;
            })("\n{\n  \"apiKey\": \"AIzaSyCqyCcs2R2e7AegGjvFAwG98wlamtbHvZY\",\n  \"authDomain\": \"bard-frontend.firebaseapp.com\",\n  \"projectId\": \"bard-frontend\",\n  \"storageBucket\": \"bard-frontend.firebasestorage.app\",\n  \"messagingSenderId\": \"175205271074\",\n  \"appId\": \"1:175205271074:web:2b7bd4d34d33bf38e6ec7b\"\n}\n","eyJhbGciOiJSUzI1NiIsImtpZCI6ImQ1OTQzZDdlNWZhYTIxZjcyOTJjMzI3MGY3ZDBjNWJmNDE5MjJmOTciLCJ0eXAiOiJKV1QifQ.eyJzdWIiOiJmaXJlYmFzZS1hZG1pbnNkay1mYnN2Y0BiYXJkLWZyb250ZW5kLmlhbS5nc2VydmljZWFjY291bnQuY29tIiwiYXVkIjoiaHR0cHM6Ly9pZGVudGl0eXRvb2xraXQuZ29vZ2xlYXBpcy5jb20vZ29vZ2xlLmlkZW50aXR5LmlkZW50aXR5dG9vbGtpdC52MS5JZGVudGl0eVRvb2xraXQiLCJ1aWQiOiIwNTk5MzQ0MTI1MjEzNjQwODM0MyIsImlzcyI6ImZpcmViYXNlLWFkbWluc2RrLWZic3ZjQGJhcmQtZnJvbnRlbmQuaWFtLmdzZXJ2aWNlYWNjb3VudC5jb20iLCJjbGFpbXMiOnsiYXBwSWQiOiJkMjE1YTVmZWMwMjItUFJfV2Fsa3Rocm91Z2hfRGlmZkxlbnMuaHRtbC02MjQifSwiZXhwIjoxNzcwMjE2ODIyLCJpYXQiOjE3NzAyMTMyMjIsImFsZyI6IlJTMjU2In0.e--Ifx2FsxcCAI5abtpGd90q8N1yuReF9mRfUEiTW4O26SfcLEwciaS7j-7eYF5wwjW_Rf38N3h2_i2GsK3fCn-t-qxby0kMS1nnvjOUBO_NQXwzphw67MY8ji9swTcqVtphGogOMoN8A2YMVK3e44Sv00eWCiMUyWW801Wz5xPYwxTgtBr9RxPkp54RO8C9n8t6prADa5dkB7kTqH_jHVuEcc-Ia9t0gwwrGx85ShGcJCr-zEUuC1VbxiAUak2ydP5vfU8yuhGJZN--5GTnYT6u8UxjPEgKwmsuVg_hOTMRoBAFLUHuhXnvBVrqY61Zyo-mOwXIuXqnM8lc3nJubw","d215a5fec022-PR_Walkthrough_DiffLens.html-624")</script><script>'use strict';var h=typeof Object.defineProperties=="function"?Object.defineProperty:function(a,b,d){if(a==Array.prototype||a==Object.prototype)return a;a[b]=d.value;return a};function l(a){a=["object"==typeof globalThis&&globalThis,a,"object"==typeof window&&window,"object"==typeof self&&self,"object"==typeof global&&global];for(var b=0;b<a.length;++b){var d=a[b];if(d&&d.Math==Math)return d}throw Error("Cannot find global object");}var n=l(this);
function p(a,b){if(b)a:{var d=n;a=a.split(".");for(var c=0;c<a.length-1;c++){var e=a[c];if(!(e in d))break a;d=d[e]}a=a[a.length-1];c=d[a];b=b(c);b!=c&&b!=null&&h(d,a,{configurable:!0,writable:!0,value:b})}}function r(a){function b(c){return a.next(c)}function d(c){return a.throw(c)}return new Promise(function(c,e){function f(g){g.done?c(g.value):Promise.resolve(g.value).then(b,d).then(f,e)}f(a.next())})}function t(a){return r(a())}
p("Object.values",function(a){return a?a:function(b){var d=[],c;for(c in b)Object.prototype.hasOwnProperty.call(b,c)&&d.push(b[c]);return d}});p("Array.prototype.includes",function(a){return a?a:function(b,d){var c=this;c instanceof String&&(c=String(c));var e=c.length;d=d||0;for(d<0&&(d=Math.max(d+e,0));d<e;d++){var f=c[d];if(f===b||Object.is(f,b))return!0}return!1}});/*

 MIT License

 Copyright (c) 2017-2023 W.Y.

 Permission is hereby granted, free of charge, to any person obtaining a copy
 of this software and associated documentation files (the "Software"), to deal
 in the Software without restriction, including without limitation the rights
 to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 copies of the Software, and to permit persons to whom the Software is
 furnished to do so, subject to the following conditions:

 The above copyright notice and this permission notice shall be included in
 all copies or substantial portions of the Software.

 THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 SOFTWARE.

*/
function u(a,b){const d=a.style;b.backgroundColor&&(d.backgroundColor=b.backgroundColor);b.width&&(d.width=`${b.width}px`);b.height&&(d.height=`${b.height}px`);const c=b.style;c!=null&&Object.keys(c).forEach(e=>{d[e]=c[e]})};var v=(()=>{let a=0;return()=>{a+=1;return`u${`0000${(Math.random()*1679616<<0).toString(36)}`.slice(-4)}${a}`}})();function w(a){const b=[];for(let d=0,c=a.length;d<c;d++)b.push(a[d]);return b}let x=null;function y(a={}){return x?x:a.l?x=a.l:x=w(window.getComputedStyle(document.documentElement))}function z(a,b){return(a=(a.ownerDocument.defaultView||window).getComputedStyle(a).getPropertyValue(b))?parseFloat(a.replace("px","")):0}
function A(a,b={}){var d;if(!(d=b.width)){d=z(a,"border-left-width");var c=z(a,"border-right-width");d=a.clientWidth+d+c}(b=b.height)||(b=z(a,"border-top-width"),c=z(a,"border-bottom-width"),b=a.clientHeight+b+c);return{width:d,height:b}}function B(a){return new Promise((b,d)=>{const c=new Image;c.onload=()=>{c.decode().then(()=>{requestAnimationFrame(()=>b(c))})};c.onerror=d;c.crossOrigin="anonymous";c.decoding="async";c.src=a})}
function C(a){return t(function*(){return Promise.resolve().then(()=>(new XMLSerializer).serializeToString(a)).then(encodeURIComponent).then(b=>`data:image/svg+xml;charset=utf-8,${b}`)})}
function D(a,b,d){return t(function*(){const c=document.createElementNS("http://www.w3.org/2000/svg","svg"),e=document.createElementNS("http://www.w3.org/2000/svg","foreignObject");c.setAttribute("width",`${b}`);c.setAttribute("height",`${d}`);c.setAttribute("viewBox",`0 0 ${b} ${d}`);e.setAttribute("width","100%");e.setAttribute("height","100%");e.setAttribute("x","0");e.setAttribute("y","0");e.setAttribute("externalResourcesRequired","true");c.appendChild(e);e.appendChild(a);return C(c)})}
var E=(a,b)=>{if(a instanceof b)return!0;a=Object.getPrototypeOf(a);return a===null?!1:a.constructor.name===b.name||E(a,b)};function F(a,b){return y(b).map(d=>{const c=a.getPropertyValue(d),e=a.getPropertyPriority(d);return`${d}: ${c}${e?" !important":""};`}).join(" ")}
function G(a,b,d,c){a=window.getComputedStyle(a,d);var e=a.getPropertyValue("content");if(e!==""&&e!=="none"){var f=v();try{b.className=`${b.className} ${f}`}catch(k){return}e=document.createElement("style");var g=e.appendChild;d=`.${f}:${d}`;a.cssText?(c=a.getPropertyValue("content"),c=`${a.cssText} content: '${c.replace(/'|"/g,"")}';`):c=F(a,c);g.call(e,document.createTextNode(`${d}{${c}}`));b.appendChild(e)}};function H(a){return a.search(/^(data:)/)!==-1}function I(a,b,d){return t(function*(){const c=yield fetch(a,b);if(c.status===404)throw Error(`Resource "${c.url}" not found`);const e=yield c.blob();return new Promise((f,g)=>{const k=new FileReader;k.onerror=g;k.onloadend=()=>{try{f(d({o:c,result:k.result}))}catch(m){g(m)}};k.readAsDataURL(e)})})}const J={};function K(a,b,d){let c=a.replace(/\?.*/,"");d&&(c=a);/ttf|otf|eot|woff2?/i.test(c)&&(c=c.replace(/.*\//,""));return b?`[${b}]${c}`:c}
function L(a,b,d){return t(function*(){const c=K(a,b,d.C);if(J[c]!=null)return J[c];d.u&&(a+=(/\?/.test(a)?"&":"?")+(new Date).getTime());let e;try{const f=yield I(a,d.i,({o:g,result:k})=>{b||(b=g.headers.get("Content-Type")||"");return k.split(/,/)[1]});e=`data:${b};base64,${f}`}catch(f){e=d.B||""}return J[c]=e})};const M={P:"application/font-woff",R:"application/font-woff",N:"application/font-truetype",v:"application/vnd.ms-fontobject",H:"image/png",F:"image/jpeg",D:"image/jpeg",A:"image/gif",M:"image/tiff",L:"image/svg+xml",O:"image/webp"};function N(a){return(a=/\.([^./]*?)$/g.exec(a))?a[1]:""};function O(a){return t(function*(){const b=a.toDataURL();return b==="data:,"?a.cloneNode(!1):B(b)})}function aa(a,b){return t(function*(){if(a.currentSrc){var d=document.createElement("canvas");const c=d.getContext("2d");d.width=a.clientWidth;d.height=a.clientHeight;c==null||c.drawImage(a,0,0,d.width,d.height);d=d.toDataURL();return B(d)}d=a.poster;d=yield L(d,M[N(d).toLowerCase()]||"",b);return B(d)})}
function ba(a,b){return t(function*(){try{let d;if(a==null?0:(d=a.contentDocument)==null?0:d.body)return yield P(a.contentDocument.body,b,!0)}catch(d){}return a.cloneNode(!1)})}function ca(a,b){return t(function*(){return E(a,HTMLCanvasElement)?O(a):E(a,HTMLVideoElement)?aa(a,b):E(a,HTMLIFrameElement)?ba(a,b):a.cloneNode(a.tagName!=null&&a.tagName.toUpperCase()==="SVG")})}
function da(a,b,d){return t(function*(){if(b.tagName!=null&&b.tagName.toUpperCase()==="SVG")return b;let c=[];if(a.tagName!=null&&a.tagName.toUpperCase()==="SLOT"&&a.assignedNodes)c=w(a.assignedNodes());else{let e;if(E(a,HTMLIFrameElement)&&((e=a.contentDocument)==null?0:e.body))c=w(a.contentDocument.body.childNodes);else{let f;c=w(((f=a.shadowRoot)!=null?f:a).childNodes)}}if(c.length===0||E(a,HTMLVideoElement))return b;yield c.reduce((e,f)=>e.then(()=>P(f,d)).then(g=>{g&&b.appendChild(g)}),Promise.resolve());
return b})}function ea(a,b,d){const c=b.style;if(c){var e=window.getComputedStyle(a);e.cssText?(c.cssText=e.cssText,c.transformOrigin=e.transformOrigin):y(d).forEach(f=>{let g=e.getPropertyValue(f);f==="font-size"&&g.endsWith("px")&&(g=`${Math.floor(parseFloat(g.substring(0,g.length-2)))-.1}px`);E(a,HTMLIFrameElement)&&f==="display"&&g==="inline"&&(g="block");f==="d"&&b.getAttribute("d")&&(g=`path(${b.getAttribute("d")})`);c.setProperty(f,g,e.getPropertyPriority(f))})}}
function fa(a,b){E(a,HTMLSelectElement)&&(b=Array.from(b.children).find(d=>a.value===d.getAttribute("value")))&&b.setAttribute("selected","")}
function ha(a,b){return t(function*(){var d=a.querySelectorAll?a.querySelectorAll("use"):[];if(d.length===0)return a;var c={};for(var e=0;e<d.length;e++){var f=d[e].getAttribute("xlink:href");if(f){const g=document.querySelector(f);a.querySelector(f)||!g||c[f]||(c[f]=yield P(g,b,!0))}}d=Object.values(c);if(d.length){c=document.createElementNS("http://www.w3.org/1999/xhtml","svg");c.setAttribute("xmlns","http://www.w3.org/1999/xhtml");c.style.position="absolute";c.style.width="0";c.style.height="0";
c.style.overflow="hidden";c.style.display="none";e=document.createElementNS("http://www.w3.org/1999/xhtml","defs");c.appendChild(e);for(f=0;f<d.length;f++)e.appendChild(d[f]);a.appendChild(c)}return a})}
function P(a,b,d){return t(function*(){return d||!b.filter||b.filter(a)?Promise.resolve(a).then(c=>ca(c,b)).then(c=>da(a,c,b)).then(c=>{E(c,Element)&&(ea(a,c,b),G(a,c,":before",b),G(a,c,":after",b),E(a,HTMLTextAreaElement)&&(c.textContent=a.value),E(a,HTMLInputElement)&&c.setAttribute("value",a.value),fa(a,c));return c}).then(c=>ha(c,b)):null})};const Q=/url\((['"]?)([^'"]+?)\1\)/g,ia=/url\([^)]+\)\s*format\((["']?)([^"']+)\1\)/g,ja=/src:\s*(?:url\([^)]+\)\s*format\([^)]+\)[,;]\s*)+/g;function ka(a){const b=[];a.replace(Q,(d,c,e)=>{b.push(e);return d});return b.filter(d=>!H(d))}
function la(a,b,d,c){return t(function*(){try{const e=d?(new URL(b,d||void 0)).toString():b;let f;f=yield L(e,M[N(b).toLowerCase()]||"",c);return a.replace(new RegExp(`(url\\(['"]?)(${b.replace(/([.*+?^${}()|\[\]\/\\])/g,"\\$1")})(['"]?\\))`,"g"),`$1${f}$3`)}catch(e){}return a})}function ma(a,{I:b}){return b?a.replace(ja,d=>{for(;;){const [c,,e]=ia.exec(d)||[],f=c,g=e;if(!g)return"";if(g===b)return`src: ${f};`}}):a}
function R(a,b,d){return t(function*(){if(a.search(Q)===-1)return a;const c=ma(a,d);return ka(c).reduce((e,f)=>e.then(g=>la(g,f,b,d)),Promise.resolve(c))})};function S(a,b,d){return t(function*(){var c;const e=(c=b.style)==null?void 0:c.getPropertyValue(a);return e?(c=yield R(e,null,d),b.style.setProperty(a,c,b.style.getPropertyPriority(a)),!0):!1})}function na(a,b){return t(function*(){(yield S("background",a,b))||(yield S("background-image",a,b));(yield S("mask",a,b))||(yield S("-webkit-mask",a,b))||(yield S("mask-image",a,b))||(yield S("-webkit-mask-image",a,b))})}
function oa(a,b){return t(function*(){const d=E(a,HTMLImageElement);if(d&&!H(a.src)||E(a,SVGImageElement)&&!H(a.href.baseVal)){var c=d?a.src:a.href.baseVal,e=yield L(c,M[N(c).toLowerCase()]||"",b);yield new Promise((f,g)=>{a.onload=f;a.onerror=b.m?(...k)=>{try{f(b.m(...k))}catch(m){g(m)}}:g;a.decode&&(a.decode=f);a.loading==="lazy"&&(a.loading="eager");d?(a.srcset="",a.src=e):a.href.baseVal=e})}})}
function pa(a,b){return t(function*(){const d=w(a.childNodes).map(c=>T(c,b));yield Promise.all(d).then(()=>a)})}function T(a,b){return t(function*(){E(a,Element)&&(yield na(a,b),yield oa(a,b),yield pa(a,b))})};const U={};function V(a){return t(function*(){var b=U[a];if(b!=null)return b;b=yield(yield fetch(a)).text();b={url:a,cssText:b};return U[a]=b})}function W(a,b){return t(function*(){let d=a.cssText;const c=/url\(["']?([^"')]+)["']?\)/g,e=(d.match(/url\([^)]+\)/g)||[]).map(f=>t(function*(){let g=f.replace(c,"$1");g.startsWith("https://")||(g=(new URL(g,a.url)).href);return I(g,b.i,({result:k})=>{d=d.replace(f,`url(${k})`);return[f,k]})}));return Promise.all(e).then(()=>d)})}
function X(a){if(a==null)return[];const b=[];a=a.replace(/(\/\*[\s\S]*?\*\/)/gi,"");for(var d=RegExp("((@.*?keyframes [\\s\\S]*?){([\\s\\S]*?}\\s*?)})","gi");;){var c=d.exec(a);if(c===null)break;b.push(c[0])}a=a.replace(d,"");d=/@import[\s\S]*?url\([^)]*\)[\s\S]*?;/gi;for(c=RegExp("((\\s*?(?:\\/\\*[\\s\\S]*?\\*\\/)?\\s*?@media[\\s\\S]*?){([\\s\\S]*?)}\\s*?})|(([\\s\\S]*?){([\\s\\S]*?)})","gi");;){let e=d.exec(a);if(e===null)if(e=c.exec(a),e===null)break;else d.lastIndex=c.lastIndex;else c.lastIndex=
d.lastIndex;b.push(e[0])}return b}
function qa(a,b){return t(function*(){const d=[],c=[];a.forEach(e=>{if("cssRules"in e)try{w(e.cssRules||[]).forEach((f,g)=>{if(f.type===CSSRule.IMPORT_RULE){let k=g+1;f=V(f.href).then(m=>W(m,b)).then(m=>X(m).forEach(q=>{try{e.insertRule(q,q.startsWith("@import")?k+=1:e.cssRules.length)}catch(Da){}})).catch(()=>{});c.push(f)}})}catch(f){const g=a.find(k=>k.href==null)||document.styleSheets[0];e.href!=null&&c.push(V(e.href).then(k=>W(k,b)).then(k=>X(k).forEach(m=>{g.insertRule(m,g.cssRules.length)})).catch(()=>
{}))}});return Promise.all(c).then(()=>{a.forEach(e=>{if("cssRules"in e)try{w(e.cssRules||[]).forEach(f=>{d.push(f)})}catch(f){}});return d})})}function ra(a){return a.filter(b=>b.type===CSSRule.FONT_FACE_RULE).filter(b=>b.style.getPropertyValue("src").search(Q)!==-1)}function sa(a,b){return t(function*(){if(a.ownerDocument==null)throw Error("Provided element is not within a Document");var d=w(a.ownerDocument.styleSheets);d=yield qa(d,b);return ra(d)})}
function ta(a){function b(c){(c.style.fontFamily||getComputedStyle(c).fontFamily).split(",").forEach(e=>{d.add(e.trim().replace(/["']/g,""))});Array.from(c.children).forEach(e=>{e instanceof HTMLElement&&b(e)})}const d=new Set;b(a);return d}function ua(a,b){return t(function*(){const d=yield sa(a,b),c=ta(a);return(yield Promise.all(d.filter(e=>c.has(e.style.fontFamily.trim().replace(/["']/g,""))).map(e=>R(e.cssText,e.parentStyleSheet?e.parentStyleSheet.href:null,b)))).join("\n")})}
function va(a,b){return t(function*(){const d=b.j!=null?b.j:b.K?null:yield ua(a,b);if(d){const c=document.createElement("style");c.appendChild(document.createTextNode(d));a.firstChild?a.insertBefore(c,a.firstChild):a.appendChild(c)}})};function wa(a,b={}){return t(function*(){const {width:d,height:c}=A(a,b),e=yield P(a,b,!0);yield va(e,b);yield T(e,b);u(e,b);return yield D(e,d,c)})}
function xa(a,b={}){return t(function*(){const {width:d,height:c}=A(a,b);var e=yield wa(a,b);e=yield B(e);const f=document.createElement("canvas"),g=f.getContext("2d"),k=b.G||window.devicePixelRatio||1,m=b.h||d,q=b.g||c;f.width=m*k;f.height=q*k;!b.J&&(f.width>16384||f.height>16384)&&(f.width>16384&&f.height>16384?f.width>f.height?(f.height*=16384/f.width,f.width=16384):(f.width*=16384/f.height,f.height=16384):f.width>16384?(f.height*=16384/f.width,f.width=16384):(f.width*=16384/f.height,f.height=
16384));f.style.width=`${m}`;f.style.height=`${q}`;b.backgroundColor&&(g.fillStyle=b.backgroundColor,g.fillRect(0,0,f.width,f.height));g.drawImage(e,0,0,f.width,f.height);return f})}function ya(a,b={}){return t(function*(){return(yield xa(a,b)).toDataURL()})};const za=["gemini.google.com","corp.google.com","proxy.googlers.com"];function Y(){return document.body.querySelectorAll('[class*="animate"]').length>0}function Z(a){return t(function*(){try{return yield ya(a,{h:a.offsetWidth,g:a.offsetHeight})}catch(d){var b=a.offsetHeight;const c=document.createElement("canvas");c.width=a.offsetWidth;c.height=b;return c.toDataURL("image/png")}})}
function Aa(){return t(function*(){const a=document.body.offsetWidth,b=document.body.offsetHeight,d=document.body.cloneNode(!0);d.querySelectorAll('[class*="animate"]').forEach(c=>{c.classList.remove(...Array.from(c.classList).filter(e=>e.startsWith("animate")))});d.style.width=`${a}px`;d.style.height=`${b}px`;return d})}
function Ba(a){return t(function*(){let b=document.body;if(Y()){var d=yield Aa();b=d;document.body.appendChild(d)}d=yield Z(b);Y()&&document.body.removeChild(b);window.parent.postMessage({type:"SEND_SCREENSHOT",image:d,topOffset:document.documentElement.scrollTop},a.origin)})}function Ca(a){return t(function*(){const b={type:"SEND_SCREENSHOT_FOR_DATA_VISUALIZATION",image:yield Z(document.body),topOffset:0};window.parent.postMessage(b,a.origin)})}
window.addEventListener("message",a=>t(function*(){if(za.some(d=>a.origin.includes(d))){var b=a.data;b&&(b.type==="MAKE_SCREENSHOT"&&(yield Ba(a)),b.type==="MAKE_SCREENSHOT_FOR_DATA_VISUALIZATION"&&(yield Ca(a)))}}));
</script><script>(function() {
  // Ensure this script is executed only once
  if (window.firebaseAuthBridgeScriptLoaded) {
    return;
  }
  window.firebaseAuthBridgeScriptLoaded = true;

  let nextTokenPromiseId = 0;

  // Stores { resolve, reject } for ongoing token requests
  const pendingTokenPromises = {};

  // Listen for messages from the Host Application
  window.addEventListener('message', function(event) {

    const messageData = event.data;

  if (messageData && messageData.type === 'RESOLVE_NEW_FIREBASE_TOKEN') {
      const { success, token, error, promiseId } = messageData ?? {};
      if (pendingTokenPromises[promiseId]) {
        if (success) {
          pendingTokenPromises[promiseId].resolve(token);
        } else {
          pendingTokenPromises[promiseId].reject(new Error(error || 'Token refresh failed from host.'));
        }
        delete pendingTokenPromises[promiseId];
      }
    }
  });

  // Expose a function for the Generated App to request a new Firebase token
  window.requestNewFirebaseToken = function() {
    const currentPromiseId = nextTokenPromiseId++;
    const promise = new Promise((resolve, reject) => {
      pendingTokenPromises[currentPromiseId] = { resolve, reject };
    });
    if (window.parent && window.parent !== window) {
      window.parent.postMessage({
        type: 'REQUEST_NEW_FIREBASE_TOKEN',
        promiseId: currentPromiseId
      }, '*');
    } else {
      pendingTokenPromises[currentPromiseId].reject(new Error('No parent window to request token from.'));
      delete pendingTokenPromises[currentPromiseId];
    }
    return promise;
  };
})();</script><script>
let realOriginalGetUserMedia = null;
if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
  realOriginalGetUserMedia = navigator.mediaDevices.getUserMedia.bind(navigator.mediaDevices);
}

(function() {
  if (navigator.mediaDevices && navigator.mediaDevices.__proto__) {
    try {
      Object.defineProperty(navigator.mediaDevices.__proto__, 'getUserMedia', {
        get: function() {
          return undefined; // Or throw an error
        },
        configurable: false
      });
    } catch (error) {
      console.error("Error defining prototype getter:", error);
    }
  }
})();

(function() {
  const pendingMediaResolvers = {};
  let nextMediaPromiseId = 0;

  function requestMediaPermissions(constraints) {
    const mediaPromiseId = nextMediaPromiseId++;
    const promise = new Promise((resolve, reject) => {
      pendingMediaResolvers[mediaPromiseId] = (granted) => {
        delete pendingMediaResolvers[mediaPromiseId];
        resolve(granted);
      };
    });

    window.parent.postMessage({
      type: 'requestMediaPermission',
      constraints: constraints,
      promiseId: mediaPromiseId,
    }, '*');

    return promise;
  }

  let originalGetUserMedia = realOriginalGetUserMedia;

  function interceptGetUserMedia() {
    if (navigator.mediaDevices) {
      Object.defineProperty(navigator.mediaDevices, 'getUserMedia', {
        value: function(constraints) {
          return requestMediaPermissions(constraints).then((granted) => {
            if (granted) {
              if (originalGetUserMedia) {
                return originalGetUserMedia(constraints);
              } else {
                throw new Error("Original getUserMedia not available.");
              }
            } else {
              throw new DOMException('Permission denied', 'NotAllowedError');
            }
          });
        },
        writable: false,
        configurable: false
      });
    }
  }

  interceptGetUserMedia();

  const observer = new MutationObserver(function(mutationsList, observer) {
    for (const mutation of mutationsList) {
      if (mutation.type === 'reconfigured' && mutation.name === 'getUserMedia' && mutation.object === navigator.mediaDevices) {
        interceptGetUserMedia();
      } else if (mutation.type === 'attributes' && mutation.attributeName === 'getUserMedia' && mutation.target === navigator.mediaDevices) {
        interceptGetUserMedia();
      } else if (mutation.type === 'childList' && mutation.addedNodes) {
        mutation.addedNodes.forEach(node => {
          if (node === navigator.mediaDevices) {
            interceptGetUserMedia();
          }
        });
      }
    }
  });

  function interceptSpeechRecognition() {
    if (!window.SpeechRecognition && !window.webkitSpeechRecognition) {
      return;
    }

    const OriginalSpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

    const SpeechRecognitionWrapper = function(...args) {
      const recognizer = new OriginalSpeechRecognition(...args);
      const originalStart = recognizer.start.bind(recognizer);

      recognizer.start = function() {
        requestMediaPermissions({ audio: true }).then(granted => {
          if (granted) {
            originalStart();
          } else {
            const errorEvent = new SpeechRecognitionErrorEvent('error');
            errorEvent.error = 'not-allowed'; // This is the standard error for permission denial.
            recognizer.dispatchEvent(errorEvent);
          }
        });
      };

      return recognizer;
    };

    SpeechRecognitionWrapper.prototype = OriginalSpeechRecognition.prototype;
    SpeechRecognitionWrapper.prototype.constructor = SpeechRecognitionWrapper;

    if (window.SpeechRecognition) {
      window.SpeechRecognition = SpeechRecognitionWrapper;
    }
    if (window.webkitSpeechRecognition) {
      window.webkitSpeechRecognition = SpeechRecognitionWrapper;
    }
  }

  interceptSpeechRecognition();

  window.addEventListener('message', function(event) {
    if (event.data) {
      if (event.data.type === 'resolveMediaPermission') {
        const { promiseId, granted } = event.data;
        if (pendingMediaResolvers[promiseId]) {
          pendingMediaResolvers[promiseId](granted);
        }
      }
    }
  });

})();</script><script>((function(modelInformation) {
  const originalFetch = window.fetch;
  // TODO: b/421908508 - Move these out of the script and match all generative AI model calls.
  let googleLlmBaseApiUrls = [
    'https://generativelanguage.googleapis.com/v1beta/models/' + modelInformation.textModelName + ':streamGenerateContent',
    'https://generativelanguage.googleapis.com/v1beta/models/' + modelInformation.textModelName + ':generateContent',
    'https://generativelanguage.googleapis.com/v1beta/models/' + modelInformation.imageModelName + ':predict',
    'https://generativelanguage.googleapis.com/v1beta/models/' + modelInformation.imageModelName + ':predictLongRunning',
    'https://generativelanguage.googleapis.com/v1beta/models/' + modelInformation.imageEditModelName + ':generateContent',
    'https://generativelanguage.googleapis.com/v1beta/models/' + modelInformation.imageTransformModelName + ':generateContent',
    'https://generativelanguage.googleapis.com/v1beta/models/' + modelInformation.videoModelName + ':predict',
    'https://generativelanguage.googleapis.com/v1beta/models/' + modelInformation.videoModelName + ':predictLongRunning',
    'https://generativelanguage.googleapis.com/v1beta/models/' + modelInformation.ttsModelName + ':generateContent',
  ];
  modelInformation.deprecatedTextModelNames.forEach((modelName) => {
    googleLlmBaseApiUrls.push(
      'https://generativelanguage.googleapis.com/v1beta/models/' + modelName + ':streamGenerateContent',
      'https://generativelanguage.googleapis.com/v1beta/models/' + modelName + ':generateContent',
    );
  });
  modelInformation.deprecatedImageModelNames.forEach((modelName) => {
    googleLlmBaseApiUrls.push(
      'https://generativelanguage.googleapis.com/v1beta/models/' + modelName + ':predict',
      'https://generativelanguage.googleapis.com/v1beta/models/' + modelName + ':predictLongRunning',
    );
  });

  const pendingFetchResolvers = {};
  let nextPromiseId = 0;

  function handleStringInput(input, optionsArgument) {
    const actualUrl = input;
    const fetchCallArgs = [actualUrl, optionsArgument];
    const effectiveOptions = optionsArgument || {};
    const bodyForApiKeyCheck = effectiveOptions.body;
    const bodyForPostMessage = effectiveOptions.body;
    return { actualUrl, fetchCallArgs, effectiveOptions, bodyForApiKeyCheck, bodyForPostMessage };
  }

  function handleRequestInput(input, optionsArgument) {
    const actualUrl = input.url;
    const fetchCallArgs = [input, optionsArgument];
    const effectiveOptions = { method: input.method, headers: new Headers(input.headers) };
    let bodyForApiKeyCheck;
    let bodyForPostMessage;

    if (optionsArgument) {
      if (optionsArgument.method) effectiveOptions.method = optionsArgument.method;
      if (optionsArgument.headers) effectiveOptions.headers = new Headers(optionsArgument.headers);
      if ('body' in optionsArgument) {
        bodyForApiKeyCheck = optionsArgument.body;
        bodyForPostMessage = optionsArgument.body;
      } else {
        bodyForApiKeyCheck = undefined;
        bodyForPostMessage = input.body;
      }
    } else {
      bodyForApiKeyCheck = undefined;
      bodyForPostMessage = input.body;
    }
    return { actualUrl, fetchCallArgs, effectiveOptions, bodyForApiKeyCheck, bodyForPostMessage };
  }

  window.fetch = function(input, optionsArgument) {
    let actualUrl;
    let fetchCallArgs;
    let effectiveOptions = {};
    let bodyForApiKeyCheck;
    let bodyForPostMessage;

    if (typeof input === 'string') {
      ({actualUrl, fetchCallArgs, effectiveOptions, bodyForApiKeyCheck, bodyForPostMessage} = handleStringInput(input, optionsArgument));
    } else if (input instanceof Request) {
      ({actualUrl, fetchCallArgs, effectiveOptions, bodyForApiKeyCheck, bodyForPostMessage} = handleRequestInput(input, optionsArgument));
    } else {
      return originalFetch.apply(window, [input, optionsArgument]);
    }

    effectiveOptions.method = effectiveOptions.method || 'GET';
    if (!effectiveOptions.headers) {
      effectiveOptions.headers = new Headers();
    }


    if (typeof actualUrl === 'string' && googleLlmBaseApiUrls.some((url) => actualUrl.startsWith(url))) {
      let apiKeyIsNull = true;

      const regex = new RegExp("models/([^:]+)");
      const modelNameMatch = actualUrl.match(regex);
      const modelName = modelNameMatch ? modelNameMatch[1] : 'unspecified';


      try {
        const urlObject = new URL(actualUrl);  // Use URL object for robust parsing
        const apiKeyParam = urlObject.searchParams.get('key');
        if (apiKeyParam) {
          apiKeyIsNull = false;
        }
      } catch (e) {
        // Continue checks even if URL parsing fails
      }

      if (apiKeyIsNull && effectiveOptions.headers) {
        const h = new Headers(effectiveOptions.headers);
        const apiKeyHeaderValue = h.get('X-API-Key') || h.get('x-api-key');
        if (apiKeyHeaderValue) {
          apiKeyIsNull = false;
          return originalFetch.apply(window, fetchCallArgs);
        }
      }

      if (apiKeyIsNull && effectiveOptions.method && ['POST', 'PUT', 'PATCH'].includes(effectiveOptions.method.toUpperCase()) && typeof bodyForApiKeyCheck === 'string') {
        try {
          const bodyData = JSON.parse(bodyForApiKeyCheck);
          if (bodyData && bodyData.apiKey) {
            apiKeyIsNull = false;
            return originalFetch.apply(window, fetchCallArgs);
          }
        } catch (e) {
          // Ignore JSON parsing errors
        }
      }

      if(apiKeyIsNull) {
        const promiseId = nextPromiseId++;
        const promise = new Promise((resolve) => {
          pendingFetchResolvers[promiseId] = (resolvedResponse) => {
            delete pendingFetchResolvers[promiseId];
            resolve(resolvedResponse);
          };
        });

        let serializedBodyForPostMessage;
        if (typeof bodyForPostMessage === 'string' || bodyForPostMessage == null) {
            serializedBodyForPostMessage = bodyForPostMessage;
        } else if (bodyForPostMessage instanceof ReadableStream) {
            serializedBodyForPostMessage = null;
        } else {
            try {
                serializedBodyForPostMessage = JSON.stringify(bodyForPostMessage);
            } catch (e) {
                serializedBodyForPostMessage = null;
            }
        }

        const messageOptions = {
            method: effectiveOptions.method,
            headers: Object.fromEntries(new Headers(effectiveOptions.headers).entries()),
            body: serializedBodyForPostMessage
        };

        window.parent.postMessage({
          type: 'requestFetch',
          url: actualUrl,
          modelName: modelName,
          options: messageOptions,
          promiseId: promiseId,
        }, '*');

        return promise;
      }
      return originalFetch.apply(window, fetchCallArgs);
    }
    return originalFetch.apply(window, fetchCallArgs);
  };

  window.addEventListener('message', function(event) {
    if (event.data && event.data.type === 'resolveFetch') {
      const { promiseId, response } = event.data;
      if (pendingFetchResolvers[promiseId]) {
        try {
          const reconstructedResponse = new Response(response.body, {
            status: response.status,
            statusText: response.statusText,
            headers: new Headers(response.headers),
          });
          pendingFetchResolvers[promiseId](reconstructedResponse);
        } catch (error) {
          pendingFetchResolvers[promiseId](new Response(null, { status: 500, statusText: "Interceptor Response Reconstruction Error" }));
        }
      }
    }
  });

}))({"textModelName":"gemini-2.5-flash-preview-09-2025","imageModelName":"imagen-4.0-generate-001","imageEditModelName":"gemini-2.5-flash-image-preview","imageTransformModelName":"gemini-3-pro-image-preview-11-2025","videoModelName":"veo-2.0-generate-001","ttsModelName":"gemini-2.5-flash-preview-tts","deprecatedTextModelNames":["gemini-2.0-flash","gemini-2.5-flash-preview-04-17","gemini-2.5-flash-preview-05-20"],"deprecatedImageModelNames":["imagen-3.0-generate-001","imagen-3.0-generate-002"]})</script><script>(function() {
  const originalConsoleLog = console.log;
  const originalConsoleError = console.error;

    /**
   * Normalizes an error event or a promise rejection reason into a structured error object.
   * @param {*} errorEventOrReason The error object or reason.
   * @return {object} Structured error data { message, name, stack }.
   */
  function getErrorObject(errorEventOrReason) {
    if (errorEventOrReason instanceof Error) {
      return {
        message: errorEventOrReason.message,
        name: errorEventOrReason.name,
        stack: errorEventOrReason.stack,
      };
    }
    // Fallback for non-Error objects.
    try {
      return {
        message: JSON.stringify(errorEventOrReason),
        name: 'UnknownErrorType',
        stack: null,
      };
    } catch (e) {
      return {
        message: String(errorEventOrReason),
        name: 'UnknownErrorTypeNonStringifiable',
        stack: null,
      };
    }
  }

  /**
   * Converts an array of arguments (from log/error) into a single string.
   * Handles Error objects specially to include their message and stack.
   * @param {Array<*>} args - Arguments passed to console methods.
   * @return {string} A string representation of the arguments.
   */
  function stringifyArgs(args) {
    return args
      .map((arg) => {
        if (arg instanceof Error) {
          const {message, stack} = arg;
          return `Error: ${message}${stack ? ('\nStack: ' + stack) : ''}`;
        }
        if (typeof arg === 'object' && arg !== null) {
          try {
            return JSON.stringify(arg);
          } catch (error) {
            return '[Circular Object]';
          }
        } else {
          return String(arg);
        }
      })
      .join(' ');
  }

  console.log = function(...args) {
    const logString = stringifyArgs(args);
    window.parent.postMessage({ type: 'log', message: logString }, '*');
    originalConsoleLog.apply(console, args);
  };

  console.error = function(...args) {
    let errorData;
    if (args.length > 0 && args[0] instanceof Error) {
      const err = args[0];
      // If the first arg is an Error, capture its details.
      errorData = {
        type: 'error',
        source: 'CONSOLE_ERROR',
        ...getErrorObject(err),
        rawArgsString: stringifyArgs(args.slice(1)),
        timestamp: new Date().toISOString(),
      };
    } else {
      // If not an Error object, treat all args as a general error message.
      errorData = {
        type: 'error',
        source: 'CONSOLE_ERROR',
        message: stringifyArgs(args),
        name: 'ConsoleLoggedError',
        stack: null,
        timestamp: new Date().toISOString(),
      };
    }
    window.parent.postMessage(errorData, '*');
    originalConsoleError.apply(console, args);
  };

  // Listen for global unhandled synchronous errors.
  window.addEventListener('error', function(event) {
    const errorDetails = event.error ? getErrorObject(event.error) : {
      message: event.message,
      name: 'GlobalError',
      stack: null,
      filename: event.filename,
      lineno: event.lineno,
      colno: event.colno,
    };

    window.parent.postMessage({
      type: 'error',
      source: 'global',
      ...errorDetails,
      message: errorDetails.message || event.message,
      timestamp: new Date().toISOString(),
    }, '*');
  });

  // Listen for unhandled promise rejections (asynchronous errors).
  window.addEventListener('unhandledrejection', function(event) {
    const errorDetails = getErrorObject(event.reason);

    window.parent.postMessage({
      type: 'error',
      source: 'unhandledrejection',
      ...errorDetails,
      message: errorDetails.message || 'Unhandled Promise Rejection',
      timestamp: new Date().toISOString(),
    }, '*');
  });

})();</script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PR Diff Lens: Retrieval API Enhancements</title>
    
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@400;500&amp;family=Inter:wght@400;500;600;700&amp;display=swap" rel="stylesheet">

    <!-- Prism.js for basic syntax (optional usage, primarily using custom renderer for Diff visual tokens) -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">

    <style>
        /* Design Tokens & Custom Overrides */
        :root {
            --bg-page: #f4f7fb;
            --bg-diff: #0d1117;
            --diff-add-bg: #2ea04326;
            --diff-add-text: #3fb950;
            --diff-del-bg: #f8514926;
            --diff-del-text: #f85149;
            --diff-meta: #8b949e;
            --accent: #6366f1; /* Indigo 600 */
        }

        body {
            font-family: 'Inter', sans-serif;
            background-color: var(--bg-page);
            color: #1f2937;
            overflow: hidden; /* Main scroll handled by panels */
            height: 100vh;
        }

        /* The Diff Font */
        .font-mono {
            font-family: 'Fira Code', monospace;
            font-variant-ligatures: none;
        }

        /* Scrollbar Styling */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }
        ::-webkit-scrollbar-track {
            background: transparent;
        }
        ::-webkit-scrollbar-thumb {
            background: #cbd5e1;
            border-radius: 4px;
        }
        #diff-panel::-webkit-scrollbar-thumb {
            background: #30363d;
        }

        /* Diff Lens Effect */
        .diff-file-section {
            transition: opacity 0.4s ease, transform 0.4s ease;
            opacity: 0.25; /* Default dimmed state */
            filter: grayscale(80%);
        }
        
        .diff-file-section.active-focus {
            opacity: 1;
            filter: grayscale(0%);
            transform: scale(1.002); /* Subtle pop */
            box-shadow: 0 4px 20px -2px rgba(0, 0, 0, 0.5);
            z-index: 10;
            border: 1px solid #30363d;
        }

        /* Step Card Active State */
        .step-card {
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            border-left: 4px solid transparent;
        }
        .step-card.active-card {
            background-color: white;
            border-left-color: var(--accent);
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
            transform: translateX(4px);
        }
        .step-card.active-card .step-number {
            background-color: var(--accent);
            color: white;
        }

        /* Progress Bar Transition */
        #progress-bar {
            transition: width 0.3s ease-out;
        }

        /* Line Styling */
        .diff-line {
            padding: 0 1rem;
            white-space: pre-wrap;
            word-break: break-all;
            line-height: 1.5;
            font-size: 0.85rem;
            display: block;
            width: 100%;
        }
        
        .diff-line-add {
            background-color: var(--diff-add-bg);
            color: var(--diff-add-text);
        }
        
        .diff-line-del {
            background-color: var(--diff-del-bg);
            color: var(--diff-del-text);
        }
        
        .diff-line-meta {
            color: var(--diff-meta);
            background-color: #161b22;
            padding-top: 0.5rem;
            padding-bottom: 0.5rem;
            font-weight: 500;
        }

        .diff-header {
            background: linear-gradient(to right, #161b22, #0d1117);
            border-bottom: 1px solid #30363d;
            padding: 12px 16px;
            position: sticky;
            top: 0;
            z-index: 20;
            font-family: 'Inter', sans-serif;
            font-size: 0.9rem;
            color: #e6edf3;
            display: flex;
            align-items: center;
            gap: 8px;
        }
    </style>
<style>*, ::before, ::after{--tw-border-spacing-x:0;--tw-border-spacing-y:0;--tw-translate-x:0;--tw-translate-y:0;--tw-rotate:0;--tw-skew-x:0;--tw-skew-y:0;--tw-scale-x:1;--tw-scale-y:1;--tw-pan-x: ;--tw-pan-y: ;--tw-pinch-zoom: ;--tw-scroll-snap-strictness:proximity;--tw-gradient-from-position: ;--tw-gradient-via-position: ;--tw-gradient-to-position: ;--tw-ordinal: ;--tw-slashed-zero: ;--tw-numeric-figure: ;--tw-numeric-spacing: ;--tw-numeric-fraction: ;--tw-ring-inset: ;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-color:rgb(59 130 246 / 0.5);--tw-ring-offset-shadow:0 0 #0000;--tw-ring-shadow:0 0 #0000;--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;--tw-blur: ;--tw-brightness: ;--tw-contrast: ;--tw-grayscale: ;--tw-hue-rotate: ;--tw-invert: ;--tw-saturate: ;--tw-sepia: ;--tw-drop-shadow: ;--tw-backdrop-blur: ;--tw-backdrop-brightness: ;--tw-backdrop-contrast: ;--tw-backdrop-grayscale: ;--tw-backdrop-hue-rotate: ;--tw-backdrop-invert: ;--tw-backdrop-opacity: ;--tw-backdrop-saturate: ;--tw-backdrop-sepia: ;--tw-contain-size: ;--tw-contain-layout: ;--tw-contain-paint: ;--tw-contain-style: }::backdrop{--tw-border-spacing-x:0;--tw-border-spacing-y:0;--tw-translate-x:0;--tw-translate-y:0;--tw-rotate:0;--tw-skew-x:0;--tw-skew-y:0;--tw-scale-x:1;--tw-scale-y:1;--tw-pan-x: ;--tw-pan-y: ;--tw-pinch-zoom: ;--tw-scroll-snap-strictness:proximity;--tw-gradient-from-position: ;--tw-gradient-via-position: ;--tw-gradient-to-position: ;--tw-ordinal: ;--tw-slashed-zero: ;--tw-numeric-figure: ;--tw-numeric-spacing: ;--tw-numeric-fraction: ;--tw-ring-inset: ;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-color:rgb(59 130 246 / 0.5);--tw-ring-offset-shadow:0 0 #0000;--tw-ring-shadow:0 0 #0000;--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;--tw-blur: ;--tw-brightness: ;--tw-contrast: ;--tw-grayscale: ;--tw-hue-rotate: ;--tw-invert: ;--tw-saturate: ;--tw-sepia: ;--tw-drop-shadow: ;--tw-backdrop-blur: ;--tw-backdrop-brightness: ;--tw-backdrop-contrast: ;--tw-backdrop-grayscale: ;--tw-backdrop-hue-rotate: ;--tw-backdrop-invert: ;--tw-backdrop-opacity: ;--tw-backdrop-saturate: ;--tw-backdrop-sepia: ;--tw-contain-size: ;--tw-contain-layout: ;--tw-contain-paint: ;--tw-contain-style: }/* ! tailwindcss v3.4.17 | MIT License | https://tailwindcss.com */*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e5e7eb}::after,::before{--tw-content:''}:host,html{line-height:1.5;-webkit-text-size-adjust:100%;-moz-tab-size:4;tab-size:4;font-family:ui-sans-serif, system-ui, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";font-feature-settings:normal;font-variation-settings:normal;-webkit-tap-highlight-color:transparent}body{margin:0;line-height:inherit}hr{height:0;color:inherit;border-top-width:1px}abbr:where([title]){-webkit-text-decoration:underline dotted;text-decoration:underline dotted}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}b,strong{font-weight:bolder}code,kbd,pre,samp{font-family:ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;font-feature-settings:normal;font-variation-settings:normal;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}table{text-indent:0;border-color:inherit;border-collapse:collapse}button,input,optgroup,select,textarea{font-family:inherit;font-feature-settings:inherit;font-variation-settings:inherit;font-size:100%;font-weight:inherit;line-height:inherit;letter-spacing:inherit;color:inherit;margin:0;padding:0}button,select{text-transform:none}button,input:where([type=button]),input:where([type=reset]),input:where([type=submit]){-webkit-appearance:button;background-color:transparent;background-image:none}:-moz-focusring{outline:auto}:-moz-ui-invalid{box-shadow:none}progress{vertical-align:baseline}::-webkit-inner-spin-button,::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}summary{display:list-item}blockquote,dd,dl,figure,h1,h2,h3,h4,h5,h6,hr,p,pre{margin:0}fieldset{margin:0;padding:0}legend{padding:0}menu,ol,ul{list-style:none;margin:0;padding:0}dialog{padding:0}textarea{resize:vertical}input::placeholder,textarea::placeholder{opacity:1;color:#9ca3af}[role=button],button{cursor:pointer}:disabled{cursor:default}audio,canvas,embed,iframe,img,object,svg,video{display:block;vertical-align:middle}img,video{max-width:100%;height:auto}[hidden]:where(:not([hidden=until-found])){display:none}.fixed{position:fixed}.relative{position:relative}.left-0{left:0px}.right-0{right:0px}.top-0{top:0px}.z-50{z-index:50}.col-span-5{grid-column:span 5 / span 5}.col-span-7{grid-column:span 7 / span 7}.mb-2{margin-bottom:0.5rem}.mb-4{margin-bottom:1rem}.mb-8{margin-bottom:2rem}.ml-2{margin-left:0.5rem}.mr-2{margin-right:0.5rem}.flex{display:flex}.grid{display:grid}.h-1{height:0.25rem}.h-16{height:4rem}.h-5{height:1.25rem}.h-full{height:100%}.h-screen{height:100vh}.h-3{height:0.75rem}.h-64{height:16rem}.min-h-full{min-height:100%}.w-0{width:0px}.w-5{width:1.25rem}.w-full{width:100%}.w-3{width:0.75rem}.flex-1{flex:1 1 0%}.cursor-pointer{cursor:pointer}.grid-cols-12{grid-template-columns:repeat(12, minmax(0, 1fr))}.flex-col{flex-direction:column}.items-start{align-items:flex-start}.items-center{align-items:center}.justify-end{justify-content:flex-end}.justify-between{justify-content:space-between}.gap-3{gap:0.75rem}.space-y-6 > :not([hidden]) ~ :not([hidden]){--tw-space-y-reverse:0;margin-top:calc(1.5rem * calc(1 - var(--tw-space-y-reverse)));margin-bottom:calc(1.5rem * var(--tw-space-y-reverse))}.space-y-8 > :not([hidden]) ~ :not([hidden]){--tw-space-y-reverse:0;margin-top:calc(2rem * calc(1 - var(--tw-space-y-reverse)));margin-bottom:calc(2rem * var(--tw-space-y-reverse))}.space-x-2 > :not([hidden]) ~ :not([hidden]){--tw-space-x-reverse:0;margin-right:calc(0.5rem * var(--tw-space-x-reverse));margin-left:calc(0.5rem * calc(1 - var(--tw-space-x-reverse)))}.overflow-hidden{overflow:hidden}.overflow-y-auto{overflow-y:auto}.rounded-lg{border-radius:0.5rem}.rounded{border-radius:0.25rem}.rounded-3xl{border-radius:1.5rem}.rounded-full{border-radius:9999px}.rounded-xl{border-radius:0.75rem}.border{border-width:1px}.border-b{border-bottom-width:1px}.border-gray-200{--tw-border-opacity:1;border-color:rgb(229 231 235 / var(--tw-border-opacity, 1))}.border-\[\#30363d\]{--tw-border-opacity:1;border-color:rgb(48 54 61 / var(--tw-border-opacity, 1))}.border-gray-100{--tw-border-opacity:1;border-color:rgb(243 244 246 / var(--tw-border-opacity, 1))}.bg-\[\#0d1117\]{--tw-bg-opacity:1;background-color:rgb(13 17 23 / var(--tw-bg-opacity, 1))}.bg-\[\#f4f7fb\]{--tw-bg-opacity:1;background-color:rgb(244 247 251 / var(--tw-bg-opacity, 1))}.bg-gray-200{--tw-bg-opacity:1;background-color:rgb(229 231 235 / var(--tw-bg-opacity, 1))}.bg-indigo-100{--tw-bg-opacity:1;background-color:rgb(224 231 255 / var(--tw-bg-opacity, 1))}.bg-indigo-600{--tw-bg-opacity:1;background-color:rgb(79 70 229 / var(--tw-bg-opacity, 1))}.bg-white\/70{background-color:rgb(255 255 255 / 0.7)}.bg-\[\#7ce38b\]{--tw-bg-opacity:1;background-color:rgb(124 227 139 / var(--tw-bg-opacity, 1))}.bg-\[\#fa7970\]{--tw-bg-opacity:1;background-color:rgb(250 121 112 / var(--tw-bg-opacity, 1))}.bg-\[\#faa356\]{--tw-bg-opacity:1;background-color:rgb(250 163 86 / var(--tw-bg-opacity, 1))}.bg-blue-50{--tw-bg-opacity:1;background-color:rgb(239 246 255 / var(--tw-bg-opacity, 1))}.bg-gray-100{--tw-bg-opacity:1;background-color:rgb(243 244 246 / var(--tw-bg-opacity, 1))}.bg-gray-50{--tw-bg-opacity:1;background-color:rgb(249 250 251 / var(--tw-bg-opacity, 1))}.bg-green-50{--tw-bg-opacity:1;background-color:rgb(240 253 244 / var(--tw-bg-opacity, 1))}.bg-purple-50{--tw-bg-opacity:1;background-color:rgb(250 245 255 / var(--tw-bg-opacity, 1))}.bg-white{--tw-bg-opacity:1;background-color:rgb(255 255 255 / var(--tw-bg-opacity, 1))}.p-2{padding:0.5rem}.p-6{padding:1.5rem}.px-4{padding-left:1rem;padding-right:1rem}.px-8{padding-left:2rem;padding-right:2rem}.py-3{padding-top:0.75rem;padding-bottom:0.75rem}.px-2{padding-left:0.5rem;padding-right:0.5rem}.py-1{padding-top:0.25rem;padding-bottom:0.25rem}.py-2{padding-top:0.5rem;padding-bottom:0.5rem}.pb-24{padding-bottom:6rem}.pb-32{padding-bottom:8rem}.pt-16{padding-top:4rem}.pt-6{padding-top:1.5rem}.font-mono{font-family:ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace}.text-lg{font-size:1.125rem;line-height:1.75rem}.text-sm{font-size:0.875rem;line-height:1.25rem}.text-xs{font-size:0.75rem;line-height:1rem}.font-bold{font-weight:700}.font-medium{font-weight:500}.font-semibold{font-weight:600}.uppercase{text-transform:uppercase}.leading-tight{line-height:1.25}.leading-relaxed{line-height:1.625}.tracking-wide{letter-spacing:0.025em}.tracking-wider{letter-spacing:0.05em}.text-gray-400{--tw-text-opacity:1;color:rgb(156 163 175 / var(--tw-text-opacity, 1))}.text-gray-500{--tw-text-opacity:1;color:rgb(107 114 128 / var(--tw-text-opacity, 1))}.text-gray-600{--tw-text-opacity:1;color:rgb(75 85 99 / var(--tw-text-opacity, 1))}.text-gray-800{--tw-text-opacity:1;color:rgb(31 41 55 / var(--tw-text-opacity, 1))}.text-indigo-700{--tw-text-opacity:1;color:rgb(67 56 202 / var(--tw-text-opacity, 1))}.text-\[\#8b949e\]{--tw-text-opacity:1;color:rgb(139 148 158 / var(--tw-text-opacity, 1))}.text-\[\#c9d1d9\]{--tw-text-opacity:1;color:rgb(201 209 217 / var(--tw-text-opacity, 1))}.text-blue-500{--tw-text-opacity:1;color:rgb(59 130 246 / var(--tw-text-opacity, 1))}.text-green-500{--tw-text-opacity:1;color:rgb(34 197 94 / var(--tw-text-opacity, 1))}.text-purple-500{--tw-text-opacity:1;color:rgb(168 85 247 / var(--tw-text-opacity, 1))}.opacity-70{opacity:0.7}.backdrop-blur-md{--tw-backdrop-blur:blur(12px);-webkit-backdrop-filter:var(--tw-backdrop-blur) var(--tw-backdrop-brightness) var(--tw-backdrop-contrast) var(--tw-backdrop-grayscale) var(--tw-backdrop-hue-rotate) var(--tw-backdrop-invert) var(--tw-backdrop-opacity) var(--tw-backdrop-saturate) var(--tw-backdrop-sepia);backdrop-filter:var(--tw-backdrop-blur) var(--tw-backdrop-brightness) var(--tw-backdrop-contrast) var(--tw-backdrop-grayscale) var(--tw-backdrop-hue-rotate) var(--tw-backdrop-invert) var(--tw-backdrop-opacity) var(--tw-backdrop-saturate) var(--tw-backdrop-sepia)}.hover\:shadow-md:hover{--tw-shadow:0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1);--tw-shadow-colored:0 4px 6px -1px var(--tw-shadow-color), 0 2px 4px -2px var(--tw-shadow-color);box-shadow:var(--tw-ring-offset-shadow, 0 0 #0000), var(--tw-ring-shadow, 0 0 #0000), var(--tw-shadow)}@media (min-width: 768px){.md\:px-8{padding-left:2rem;padding-right:2rem}}@media (min-width: 1024px){.lg\:col-span-4{grid-column:span 4 / span 4}.lg\:col-span-8{grid-column:span 8 / span 8}}</style></head>
<body class="flex flex-col h-screen">

    <!-- Header -->
    <header class="fixed top-0 left-0 right-0 h-16 z-50 backdrop-blur-md bg-white/70 border-b border-gray-200 flex flex-col justify-end">
        <div class="flex items-center justify-between px-8 py-3 h-full">
            <div class="flex items-center gap-3">
                <div class="bg-indigo-100 text-indigo-700 p-2 rounded-lg">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 7h12m0 0l-4-4m4 4l-4 4m0 6H4m0 0l4 4m-4-4l4-4"></path>
                    </svg>
                </div>
                <div>
                    <h1 class="font-bold text-gray-800 text-lg leading-tight">PR Walkthrough</h1>
                    <p class="text-xs text-gray-500 font-medium tracking-wide uppercase">Prompts fine-tuned and chat compound query response modified</p>
                </div>
            </div>
            <div class="text-sm font-semibold text-gray-600">
                <span id="current-step-display">2</span> / <span id="total-steps-display">6</span>
            </div>
        </div>
        <!-- Progress Bar -->
        <div class="w-full h-1 bg-gray-200">
            <div id="progress-bar" class="h-full bg-indigo-600 w-0" style="width: 33.3333%;"></div>
        </div>
    </header>

    <!-- Main Layout -->
    <main class="flex-1 grid grid-cols-12 h-full pt-16">
        
        <!-- Left Panel: The Context -->
        <div id="context-panel" class="col-span-5 lg:col-span-4 bg-[#f4f7fb] overflow-y-auto p-6 pb-24 space-y-6">
            <div class="text-sm font-bold text-gray-400 uppercase tracking-wider mb-2">Change Timeline</div>
            <!-- Cards will be injected here -->
        
                <div id="step-0" class="step-card bg-white p-6 rounded-xl cursor-pointer hover:shadow-md" onclick="activateStep(0)">
                    <div class="flex justify-between items-start mb-2">
                        <span class="step-number text-xs font-bold text-gray-400 bg-gray-100 px-2 py-1 rounded-full mb-2">#1</span>
                        <span class="text-xs font-semibold px-2 py-1 rounded-full text-green-500 bg-green-50">Logic Fix &amp; Robustness</span>
                    </div>
                    <h3 class="font-bold text-gray-800 text-lg mb-2">Robust JSON Parsing for Sub-query Decomposition</h3>
                    <p class="text-gray-600 text-sm leading-relaxed mb-4">Replaces fragile string splitting logic (splitting by newline/period) with a structured JSON parsing approach. The _decompose_query method now expects a JSON list from the LLM and includes error handling (JSONDecodeError) to ensure downstream processing receives a valid list of strings. This aligns with the prompt changes enforcing JSON output.</p>
                    <div class="flex items-center text-xs text-gray-400 font-mono bg-gray-50 p-2 rounded border border-gray-100">
                        <svg class="w-3 h-3 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg>
                        RAGGenerator._decompose_query / get_subqueries
                    </div>
                </div>
                
                <div id="step-1" class="step-card bg-white p-6 rounded-xl cursor-pointer hover:shadow-md active-card" onclick="activateStep(1)">
                    <div class="flex justify-between items-start mb-2">
                        <span class="step-number text-xs font-bold text-gray-400 bg-gray-100 px-2 py-1 rounded-full mb-2">#2</span>
                        <span class="text-xs font-semibold px-2 py-1 rounded-full text-gray-500 bg-gray-50">Prompt Engineering</span>
                    </div>
                    <h3 class="font-bold text-gray-800 text-lg mb-2">Sub-query Prompt Tuning &amp; JSON Enforcement</h3>
                    <p class="text-gray-600 text-sm leading-relaxed mb-4">Heavily modifies the system prompt to enforce strict JSON array output (["Q1", "Q2"]) to support the new parsing logic in generate.py. It also increases the requested sub-query count from 6-8 to 10-15 and adds specific domain context ('Oil and Gas') to drive deeper research.</p>
                    <div class="flex items-center text-xs text-gray-400 font-mono bg-gray-50 p-2 rounded border border-gray-100">
                        <svg class="w-3 h-3 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg>
                        System Prompt (Subquery)
                    </div>
                </div>
                
                <div id="step-2" class="step-card bg-white p-6 rounded-xl cursor-pointer hover:shadow-md" onclick="activateStep(2)">
                    <div class="flex justify-between items-start mb-2">
                        <span class="step-number text-xs font-bold text-gray-400 bg-gray-100 px-2 py-1 rounded-full mb-2">#3</span>
                        <span class="text-xs font-semibold px-2 py-1 rounded-full text-blue-500 bg-blue-50">Refactor &amp; Feature Update</span>
                    </div>
                    <h3 class="font-bold text-gray-800 text-lg mb-2">V2 Compound Query Pipeline Integration</h3>
                    <p class="text-gray-600 text-sm leading-relaxed mb-4">Updates the orchestration logic in _handle_compound_query. It transitions to V2 methods (_handle_sub_queries_v2) and introduces a new step to build a global_citation_map and process source text before synthesis. This ensures that the final answer has access to consolidated citation data across all sub-queries.</p>
                    <div class="flex items-center text-xs text-gray-400 font-mono bg-gray-50 p-2 rounded border border-gray-100">
                        <svg class="w-3 h-3 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg>
                        RAGGenerator._handle_compound_query
                    </div>
                </div>
                
                <div id="step-3" class="step-card bg-white p-6 rounded-xl cursor-pointer hover:shadow-md" onclick="activateStep(3)">
                    <div class="flex justify-between items-start mb-2">
                        <span class="step-number text-xs font-bold text-gray-400 bg-gray-100 px-2 py-1 rounded-full mb-2">#4</span>
                        <span class="text-xs font-semibold px-2 py-1 rounded-full text-gray-500 bg-gray-50">Logic Update</span>
                    </div>
                    <h3 class="font-bold text-gray-800 text-lg mb-2">New V2 Synthesis Logic</h3>
                    <p class="text-gray-600 text-sm leading-relaxed mb-4">Implements _chat_synthesize_subanswers_v2, which formats the prompt input to include both sources_text (for citations) and subquery_answers_text. This prepares the context for the newly added chat_synthesizer_v2 prompt, replacing the previous simple concatenation strategy.</p>
                    <div class="flex items-center text-xs text-gray-400 font-mono bg-gray-50 p-2 rounded border border-gray-100">
                        <svg class="w-3 h-3 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg>
                        RAGGenerator._chat_synthesize_subanswers_v2
                    </div>
                </div>
                
                <div id="step-4" class="step-card bg-white p-6 rounded-xl cursor-pointer hover:shadow-md" onclick="activateStep(4)">
                    <div class="flex justify-between items-start mb-2">
                        <span class="step-number text-xs font-bold text-gray-400 bg-gray-100 px-2 py-1 rounded-full mb-2">#5</span>
                        <span class="text-xs font-semibold px-2 py-1 rounded-full text-purple-500 bg-purple-50">New Asset</span>
                    </div>
                    <h3 class="font-bold text-gray-800 text-lg mb-2">New Chat Synthesis Prompt (V2)</h3>
                    <p class="text-gray-600 text-sm leading-relaxed mb-4">Introduces a new prompt template focused on integrating sub-question answers into a coherent response with strict citation guidelines ([n] style) and requirements for numerical/statistical insights. This drives the 'chat compound query response modified' objective.</p>
                    <div class="flex items-center text-xs text-gray-400 font-mono bg-gray-50 p-2 rounded border border-gray-100">
                        <svg class="w-3 h-3 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg>
                        System Prompt (Chat Synthesizer V2)
                    </div>
                </div>
                
                <div id="step-5" class="step-card bg-white p-6 rounded-xl cursor-pointer hover:shadow-md" onclick="activateStep(5)">
                    <div class="flex justify-between items-start mb-2">
                        <span class="step-number text-xs font-bold text-gray-400 bg-gray-100 px-2 py-1 rounded-full mb-2">#6</span>
                        <span class="text-xs font-semibold px-2 py-1 rounded-full text-gray-500 bg-gray-50">Prompt Engineering</span>
                    </div>
                    <h3 class="font-bold text-gray-800 text-lg mb-2">Report Summarizer Overhaul</h3>
                    <p class="text-gray-600 text-sm leading-relaxed mb-4">Refactors the report prompt from a conversational style to a 'deeply analytical research report'. It adds specific structure requirements (Executive Summary, Data Tables), increases target word count (2500-4000), and emphasizes 'Oil and Gas' domain knowledge.</p>
                    <div class="flex items-center text-xs text-gray-400 font-mono bg-gray-50 p-2 rounded border border-gray-100">
                        <svg class="w-3 h-3 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg>
                        System Prompt (Report Summarizer)
                    </div>
                </div>
                <div class="h-64"></div></div>

        <!-- Right Panel: The Diff -->
        <div id="diff-panel" class="col-span-7 lg:col-span-8 bg-[#0d1117] overflow-y-auto relative">
            <div id="diff-content" class="min-h-full pb-32 pt-6 px-4 md:px-8 space-y-8"><div id="file-core-retrieval_api-generate-py" data-filepath="core/retrieval_api/generate.py" class="diff-file-section rounded-3xl border border-[#30363d] overflow-hidden mb-8 bg-[#0d1117]">
                        <div class="diff-header">
                            <div class="flex space-x-2">
                                <div class="w-3 h-3 rounded-full bg-[#fa7970]"></div>
                                <div class="w-3 h-3 rounded-full bg-[#faa356]"></div>
                                <div class="w-3 h-3 rounded-full bg-[#7ce38b]"></div>
                            </div>
                            <span class="ml-2 font-mono text-xs opacity-70">core/retrieval_api/generate.py</span>
                        </div>
                        <div class="code-body font-mono text-sm py-2">
                    <span class="diff-line diff-line-meta">index 6dc7415..daac475 100644</span><span class="diff-line diff-line-meta">--- a/core/retrieval_api/generate.py</span><span class="diff-line diff-line-meta">+++ b/core/retrieval_api/generate.py</span><span class="diff-line diff-line-meta text-[#8b949e]">@@ -605,21 +605,57 @@ def _retrieve_documents(self):</span><span class="diff-line text-[#c9d1d9]">      def _has_valid_docs(self, docs):</span><span class="diff-line text-[#c9d1d9]">          return docs and max([doc.score for doc in docs]) &gt; self._config("RAG_SIMILARITY_CUTOFF")</span><span class="diff-line text-[#c9d1d9]">  </span><span class="diff-line diff-line-add">+    def _decompose_query(self, query: str) -&gt; List[str]:</span><span class="diff-line diff-line-add">+        try:</span><span class="diff-line diff-line-add">+            chat_sub_query_prompt = self._prompts.chat_subquery.format(query =query)</span><span class="diff-line diff-line-add">+            response = self._model_manager.llm.complete(chat_sub_query_prompt).text.strip()</span><span class="diff-line diff-line-add">+            subqueries = self._parse_subqueries(response)</span><span class="diff-line diff-line-add">+            if isinstance(subqueries, list) and all(isinstance(q, str) for q in subqueries):</span><span class="diff-line diff-line-add">+                return subqueries</span><span class="diff-line diff-line-add">+            else:</span><span class="diff-line diff-line-add">+                raise ValueError("Invalid format: not a list of strings")</span><span class="diff-line diff-line-add">+        except json.JSONDecodeError as e:</span><span class="diff-line diff-line-add">+            raise ValueError(f"Subquery response not valid JSON: {e}")</span><span class="diff-line diff-line-add">+</span><span class="diff-line text-[#c9d1d9]">      def get_subqueries(self, main_query: str, first_pass_answer:str):</span><span class="diff-line diff-line-del">-        logger.info("getting sub queries")</span><span class="diff-line diff-line-del">-        response = self._model_manager.llm.complete(</span><span class="diff-line diff-line-del">-                self._prompts.subquery.format(query=main_query,first_pass_answer=first_pass_answer)</span><span class="diff-line diff-line-del">-            ).text.strip()</span><span class="diff-line diff-line-del">-        logger.info("getting sub queries")</span><span class="diff-line diff-line-del">-        # Parse sub-queries from numbered output</span><span class="diff-line diff-line-del">-        lines = response.strip().split("\n")</span><span class="diff-line diff-line-del">-        subqueries = []</span><span class="diff-line diff-line-del">-        for line in lines:</span><span class="diff-line diff-line-del">-            if "." in line:</span><span class="diff-line diff-line-del">-                subqueries.append(line.split(".", 1)[1].strip())</span><span class="diff-line diff-line-add">+        try:</span><span class="diff-line diff-line-add">+            logger.info("getting sub queries")</span><span class="diff-line diff-line-add">+            response = self._model_manager.llm.complete(</span><span class="diff-line diff-line-add">+                    self._prompts.subquery.format(query=main_query,first_pass_answer=first_pass_answer)</span><span class="diff-line diff-line-add">+                ).text.strip()</span><span class="diff-line diff-line-add">+            subqueries = self._parse_subqueries(response)</span><span class="diff-line diff-line-add">+            logger.info("subqueries, {subqueries}")</span><span class="diff-line diff-line-add">+            if isinstance(subqueries, list) and all(isinstance(q, str) for q in subqueries):</span><span class="diff-line diff-line-add">+                    return subqueries</span><span class="diff-line text-[#c9d1d9]">             else:</span><span class="diff-line diff-line-del">-                subqueries.append(line.strip())</span><span class="diff-line diff-line-del">-        return subqueries</span><span class="diff-line diff-line-add">+                raise ValueError("Invalid format: not a list of strings")</span><span class="diff-line diff-line-add">+        except json.JSONDecodeError as e:</span><span class="diff-line diff-line-add">+            raise ValueError(f"Subquery response not valid JSON: {e}")</span><span class="diff-line diff-line-add">+        # # Parse sub-queries from numbered output</span><span class="diff-line diff-line-add">+        # lines = response.strip().split("\n")</span><span class="diff-line diff-line-add">+        # subqueries = []</span><span class="diff-line diff-line-add">+        # for line in lines:</span><span class="diff-line diff-line-add">+        #     if "." in line:</span><span class="diff-line diff-line-add">+        #         subqueries.append(line.split(".", 1)[1].strip())</span><span class="diff-line diff-line-add">+        #     else:</span><span class="diff-line diff-line-add">+        #         subqueries.append(line.strip())</span><span class="diff-line diff-line-add">+        # return subqueries</span><span class="diff-line diff-line-add">+</span><span class="diff-line diff-line-add">+    # def get_subqueries(self, main_query: str, first_pass_answer:str):</span><span class="diff-line diff-line-add">+    #     logger.info("getting sub queries")</span><span class="diff-line diff-line-add">+    #     response = self._model_manager.llm.complete(</span><span class="diff-line diff-line-add">+    #             self._prompts.subquery.format(query=main_query,first_pass_answer=first_pass_answer)</span><span class="diff-line diff-line-add">+    #         ).text.strip()</span><span class="diff-line diff-line-add">+    #     logger.info("getting sub queries")</span><span class="diff-line diff-line-add">+    #     # Parse sub-queries from numbered output</span><span class="diff-line diff-line-add">+    #     lines = response.strip().split("\n")</span><span class="diff-line diff-line-add">+    #     subqueries = []</span><span class="diff-line diff-line-add">+    #     for line in lines:</span><span class="diff-line diff-line-add">+    #         if "." in line:</span><span class="diff-line diff-line-add">+    #             subqueries.append(line.split(".", 1)[1].strip())</span><span class="diff-line diff-line-add">+    #         else:</span><span class="diff-line diff-line-add">+    #             subqueries.append(line.strip())</span><span class="diff-line diff-line-add">+    #     return subqueries</span><span class="diff-line text-[#c9d1d9]">      </span><span class="diff-line text-[#c9d1d9]">      def _handle_compound_query_response(self):</span><span class="diff-line text-[#c9d1d9]">          query_b = QueryBundle(query_str=self._refined_query)</span><span class="diff-line diff-line-meta text-[#8b949e]">@@ -687,16 +723,20 @@ def _yield_response(self, response):</span><span class="diff-line text-[#c9d1d9]">              if chunk != "Empty Response":</span><span class="diff-line text-[#c9d1d9]">                  answer += chunk</span><span class="diff-line text-[#c9d1d9]">              yield chunk_json</span><span class="diff-line diff-line-del">-    </span><span class="diff-line diff-line-add">+</span><span class="diff-line text-[#c9d1d9]">      def _handle_compound_query(self):</span><span class="diff-line text-[#c9d1d9]">          query_b = QueryBundle(query_str=self._refined_query)</span><span class="diff-line text-[#c9d1d9]">          sub_queries = self._decompose_query(query_b)</span><span class="diff-line text-[#c9d1d9]">          if sub_queries:</span><span class="diff-line text-[#c9d1d9]">              logger.info(f"sub_queries, {sub_queries}")</span><span class="diff-line diff-line-del">-            subq_answers, retrieved_docs, subq_source_nodes = self._handle_sub_queries(sub_queries)</span><span class="diff-line diff-line-add">+            subq_answers, retrieved_docs, subq_source_nodes = self._handle_sub_queries_v2(sub_queries)</span><span class="diff-line diff-line-add">+            global_citation_map = self._build_citationmap(retrieved_docs)</span><span class="diff-line diff-line-add">+            updated_subq_answers=self._process_sub_queries(sub_queries,subq_answers,subq_source_nodes,global_citation_map)</span><span class="diff-line diff-line-add">+            sources_text = self._inputs_for_process_cntxt(global_citation_map)</span><span class="diff-line text-[#c9d1d9]">              logger.info(f"final_answer, {subq_answers}")</span><span class="diff-line text-[#c9d1d9]">              # response = self._chat_synthesize_response(query_b, subq_answer)</span><span class="diff-line diff-line-del">-            response = self._chat_synthesize_subanswers(query_b, subq_answers)</span><span class="diff-line diff-line-add">+            response = self._chat_synthesize_subanswers_v2(self._refined_query, updated_subq_answers, sources_text)</span><span class="diff-line diff-line-add">+            # response = self._chat_synthesize_subanswers(query_b, subq_answers)</span><span class="diff-line text-[#c9d1d9]">              answer = ""</span><span class="diff-line text-[#c9d1d9]">  </span><span class="diff-line text-[#c9d1d9]">              for chunk_json in self.response_generator(response.text):</span><span class="diff-line diff-line-meta text-[#8b949e]">@@ -846,29 +886,29 @@ def _inputs_for_process_cntxt(self,citation_map):</span><span class="diff-line text-[#c9d1d9]">          )</span><span class="diff-line text-[#c9d1d9]">          return sources_text</span><span class="diff-line text-[#c9d1d9]">  </span><span class="diff-line diff-line-del">-    def _chat_synthesize_response(self, main_qb:str, subquery_results:List):</span><span class="diff-line diff-line-add">+    # def _chat_synthesize_response(self, main_qb:str, subquery_results:List):</span><span class="diff-line text-[#c9d1d9]">  </span><span class="diff-line diff-line-del">-        chat_synthesize_prompt = self._prompts.chat_synthesizer.format(query=main_qb,combined_text = subquery_results)</span><span class="diff-line diff-line-del">-        response_synth = get_response_synthesizer(</span><span class="diff-line diff-line-del">-            response_mode="compact",</span><span class="diff-line diff-line-del">-            text_qa_template=chat_synthesize_prompt,</span><span class="diff-line diff-line-del">-            llm=self._model_manager.llm</span><span class="diff-line diff-line-del">-        )</span><span class="diff-line diff-line-add">+    #     chat_synthesize_prompt = self._prompts.chat_synthesizer.format(query=main_qb,combined_text = subquery_results)</span><span class="diff-line diff-line-add">+    #     response_synth = get_response_synthesizer(</span><span class="diff-line diff-line-add">+    #         response_mode="compact",</span><span class="diff-line diff-line-add">+    #         text_qa_template=chat_synthesize_prompt,</span><span class="diff-line diff-line-add">+    #         llm=self._model_manager.llm</span><span class="diff-line diff-line-add">+    #     )</span><span class="diff-line text-[#c9d1d9]">  </span><span class="diff-line diff-line-del">-        subq_nodes = [NodeWithScore(node=TextNode(text=chunk)) for chunk in subquery_results]</span><span class="diff-line diff-line-del">-        # subq_nodes = [TextNode(text=chunk) for chunk in subquery_results]</span><span class="diff-line diff-line-add">+    #     subq_nodes = [NodeWithScore(node=TextNode(text=chunk)) for chunk in subquery_results]</span><span class="diff-line diff-line-add">+    #     # subq_nodes = [TextNode(text=chunk) for chunk in subquery_results]</span><span class="diff-line text-[#c9d1d9]">  </span><span class="diff-line diff-line-del">-        response = response_synth.synthesize(main_qb, nodes=subq_nodes)</span><span class="diff-line diff-line-del">-        # response = self._model_manager.llm.complete(</span><span class="diff-line diff-line-del">-        #         self._prompts.report_summarizer.format(query=main_query,combined_text = subquery_results)</span><span class="diff-line diff-line-del">-        #     )</span><span class="diff-line diff-line-del">-        return response</span><span class="diff-line diff-line-add">+    #     response = response_synth.synthesize(main_qb, nodes=subq_nodes)</span><span class="diff-line diff-line-add">+    #     # response = self._model_manager.llm.complete(</span><span class="diff-line diff-line-add">+    #     #         self._prompts.report_summarizer.format(query=main_query,combined_text = subquery_results)</span><span class="diff-line diff-line-add">+    #     #     )</span><span class="diff-line diff-line-add">+    #     return response</span><span class="diff-line text-[#c9d1d9]">      </span><span class="diff-line diff-line-del">-    def synthesize_markdown_report(self, main_query:str, subquery_results:List):</span><span class="diff-line diff-line-del">-        response = self._model_manager.llm.complete(</span><span class="diff-line diff-line-del">-                self._prompts.report_summarizer.format(query=main_query, combined_text = subquery_results)</span><span class="diff-line diff-line-del">-            )</span><span class="diff-line diff-line-del">-        return response</span><span class="diff-line diff-line-add">+    # def synthesize_markdown_report(self, main_query:str, subquery_results:List):</span><span class="diff-line diff-line-add">+    #     response = self._model_manager.llm.complete(</span><span class="diff-line diff-line-add">+    #             self._prompts.report_summarizer.format(query=main_query, combined_text = subquery_results)</span><span class="diff-line diff-line-add">+    #         )</span><span class="diff-line diff-line-add">+    #     return response</span><span class="diff-line text-[#c9d1d9]">      </span><span class="diff-line text-[#c9d1d9]">      def _synthesize_markdown_report_v2(self, main_query:str, subquery_answers:Dict, sources_text:List):</span><span class="diff-line text-[#c9d1d9]">          logger.info(f"synthesizing report")</span><span class="diff-line diff-line-meta text-[#8b949e]">@@ -887,6 +927,20 @@ def _chat_synthesize_subanswers(self, main_query:str, subquery_results:List):</span><span class="diff-line text-[#c9d1d9]">              )</span><span class="diff-line text-[#c9d1d9]">          return response</span><span class="diff-line text-[#c9d1d9]">      </span><span class="diff-line diff-line-add">+    def _chat_synthesize_subanswers_v2(self, main_query:str, subquery_answers:Dict,sources_text:List):</span><span class="diff-line diff-line-add">+        logger.info(f"synthesizing report")</span><span class="diff-line diff-line-add">+        subquery_answers_text = "\n\n".join(</span><span class="diff-line diff-line-add">+            [f"Subquery {i+1}: {subquery}\nAnswer: {subquery_answers[subquery]}" </span><span class="diff-line diff-line-add">+            for i, subquery in enumerate(subquery_answers)]</span><span class="diff-line diff-line-add">+        )</span><span class="diff-line diff-line-add">+        response = self._model_manager.llm.complete(</span><span class="diff-line diff-line-add">+                self._prompts.chat_synthesizer_v2.format(query=main_query,sources_text = sources_text, subquery_answers_text = subquery_answers_text)</span><span class="diff-line diff-line-add">+            )</span><span class="diff-line diff-line-add">+        # response = self._model_manager.llm.complete(</span><span class="diff-line diff-line-add">+        #         self._prompts.chat_synthesizer.format(query=main_query,subquery_results = subquery_results)</span><span class="diff-line diff-line-add">+        #     )</span><span class="diff-line diff-line-add">+        return response</span><span class="diff-line diff-line-add">+    </span><span class="diff-line text-[#c9d1d9]">      def response_generator(self, response_str, chunk_size=50):</span><span class="diff-line text-[#c9d1d9]">          for i in range(0, len(response_str), chunk_size):</span><span class="diff-line text-[#c9d1d9]">              chunk = response_str[i : i + chunk_size]</span><span class="diff-line diff-line-meta text-[#8b949e]">@@ -1069,41 +1123,3 @@ def _generate_conversation_title(self):</span><span class="diff-line text-[#c9d1d9]">                  "type": "title",</span><span class="diff-line text-[#c9d1d9]">                  "text": "Untitled Conversation"</span><span class="diff-line text-[#c9d1d9]">              })</span><span class="diff-line diff-line-del">-</span><span class="diff-line diff-line-del">-            </span><span class="diff-line diff-line-del">-        </span><span class="diff-line diff-line-del">-from llama_index.core.schema import QueryBundle</span><span class="diff-line diff-line-del">-from llama_index.core.query_engine import CitationQueryEngine</span><span class="diff-line diff-line-del">-</span><span class="diff-line diff-line-del">-class CompoundQueryHandler:</span><span class="diff-line diff-line-del">-    def __init__(self, query_engine: CitationQueryEngine):</span><span class="diff-line diff-line-del">-        self.query_engine = query_engine</span><span class="diff-line diff-line-del">-</span><span class="diff-line diff-line-del">-    def decompose_query(self, query: str) -&gt; list[str]:</span><span class="diff-line diff-line-del">-        # Naive logic, replace with LLM if needed</span><span class="diff-line diff-line-del">-        if " and " in query:</span><span class="diff-line diff-line-del">-            return query.split(" and ")</span><span class="diff-line diff-line-del">-        elif " then " in query:</span><span class="diff-line diff-line-del">-            return query.split(" then ")</span><span class="diff-line diff-line-del">-        else:</span><span class="diff-line diff-line-del">-            return [query]</span><span class="diff-line diff-line-del">-</span><span class="diff-line diff-line-del">-    def handle_query(self, query: str):</span><span class="diff-line diff-line-del">-        sub_queries = self.decompose_query(query)</span><span class="diff-line diff-line-del">-        all_docs = []</span><span class="diff-line diff-line-del">-        all_responses = []</span><span class="diff-line diff-line-del">-</span><span class="diff-line diff-line-del">-        for sub_query in sub_queries:</span><span class="diff-line diff-line-del">-            sub_query = sub_query.strip()</span><span class="diff-line diff-line-del">-            query_bundle = QueryBundle(sub_query)</span><span class="diff-line diff-line-del">-</span><span class="diff-line diff-line-del">-            # Step 1: Retrieve relevant nodes</span><span class="diff-line diff-line-del">-            retrieved_docs = self.query_engine.retrieve(query_bundle)</span><span class="diff-line diff-line-del">-            all_docs.extend(retrieved_docs)</span><span class="diff-line diff-line-del">-</span><span class="diff-line diff-line-del">-            # Step 2: Synthesize the response</span><span class="diff-line diff-line-del">-            response = self.query_engine.synthesize(query_bundle, retrieved_docs)</span><span class="diff-line diff-line-del">-            all_responses.append(f"**Q:** {sub_query}\n**A:** {str(response)}")</span><span class="diff-line diff-line-del">-</span><span class="diff-line diff-line-del">-        final_response = "\n\n".join(all_responses)</span><span class="diff-line diff-line-del">-        return final_response, all_docs</span></div></div><div id="file-core-retrieval_api-managers-prompt_manager-py" data-filepath="core/retrieval_api/managers/prompt_manager.py" class="diff-file-section rounded-3xl border border-[#30363d] overflow-hidden mb-8 bg-[#0d1117]">
                        <div class="diff-header">
                            <div class="flex space-x-2">
                                <div class="w-3 h-3 rounded-full bg-[#fa7970]"></div>
                                <div class="w-3 h-3 rounded-full bg-[#faa356]"></div>
                                <div class="w-3 h-3 rounded-full bg-[#7ce38b]"></div>
                            </div>
                            <span class="ml-2 font-mono text-xs opacity-70">core/retrieval_api/managers/prompt_manager.py</span>
                        </div>
                        <div class="code-body font-mono text-sm py-2">
                    <span class="diff-line diff-line-meta">index ed8c09b..ed5b24c 100644</span><span class="diff-line diff-line-meta">--- a/core/retrieval_api/managers/prompt_manager.py</span><span class="diff-line diff-line-meta">+++ b/core/retrieval_api/managers/prompt_manager.py</span><span class="diff-line diff-line-meta text-[#8b949e]">@@ -22,6 +22,7 @@ class PromptConfig(BaseModel):</span><span class="diff-line text-[#c9d1d9]">      report_summarizer: str = "report_summarizer.prompt"</span><span class="diff-line text-[#c9d1d9]">      subquery: str = "subquery.prompt"</span><span class="diff-line text-[#c9d1d9]">      chat_synthesizer: str = "chat_synthesizer.prompt"</span><span class="diff-line diff-line-add">+    chat_synthesizer_v2: str = "chat_synthesizer_v2.prompt"</span><span class="diff-line text-[#c9d1d9]">      chat_subquery:str = "chat_subquery.prompt"</span><span class="diff-line text-[#c9d1d9]">      is_compound_prompt:str = "is_compound_query.prompt"</span><span class="diff-line text-[#c9d1d9]">      class Config:</span></div></div><div id="file-core-retrieval_api-prompts-chat_subquery-prompt" data-filepath="core/retrieval_api/prompts/chat_subquery.prompt" class="diff-file-section rounded-3xl border border-[#30363d] overflow-hidden mb-8 bg-[#0d1117]">
                        <div class="diff-header">
                            <div class="flex space-x-2">
                                <div class="w-3 h-3 rounded-full bg-[#fa7970]"></div>
                                <div class="w-3 h-3 rounded-full bg-[#faa356]"></div>
                                <div class="w-3 h-3 rounded-full bg-[#7ce38b]"></div>
                            </div>
                            <span class="ml-2 font-mono text-xs opacity-70">core/retrieval_api/prompts/chat_subquery.prompt</span>
                        </div>
                        <div class="code-body font-mono text-sm py-2">
                    <span class="diff-line diff-line-meta">index e73b0f3..6c5c778 100644</span><span class="diff-line diff-line-meta">--- a/core/retrieval_api/prompts/chat_subquery.prompt</span><span class="diff-line diff-line-meta">+++ b/core/retrieval_api/prompts/chat_subquery.prompt</span><span class="diff-line diff-line-meta text-[#8b949e]">@@ -31,3 +31,4 @@ Only return output in this format:</span><span class="diff-line text-[#c9d1d9]">    ...</span><span class="diff-line text-[#c9d1d9]">  ]</span><span class="diff-line text-[#c9d1d9]">  </span><span class="diff-line diff-line-add">+&lt;|SUBQUERIES|&gt;:</span><span class="diff-line text-[#c9d1d9]">\ No newline at end of file</span></div></div><div id="file-core-retrieval_api-prompts-chat_synthesizer_v2-prompt" data-filepath="core/retrieval_api/prompts/chat_synthesizer_v2.prompt" class="diff-file-section rounded-3xl border border-[#30363d] overflow-hidden mb-8 bg-[#0d1117]">
                        <div class="diff-header">
                            <div class="flex space-x-2">
                                <div class="w-3 h-3 rounded-full bg-[#fa7970]"></div>
                                <div class="w-3 h-3 rounded-full bg-[#faa356]"></div>
                                <div class="w-3 h-3 rounded-full bg-[#7ce38b]"></div>
                            </div>
                            <span class="ml-2 font-mono text-xs opacity-70">core/retrieval_api/prompts/chat_synthesizer_v2.prompt</span>
                        </div>
                        <div class="code-body font-mono text-sm py-2">
                    <span class="diff-line text-[#c9d1d9]">new file mode 100644</span><span class="diff-line diff-line-meta">index 0000000..ee42d1e</span><span class="diff-line diff-line-meta">--- /dev/null</span><span class="diff-line diff-line-meta">+++ b/core/retrieval_api/prompts/chat_synthesizer_v2.prompt</span><span class="diff-line diff-line-meta text-[#8b949e]">@@ -0,0 +1,33 @@</span><span class="diff-line diff-line-add">+You are an expert technical assistant that synthesizes answers to a main question by integrating responses from several focused sub-questions.</span><span class="diff-line diff-line-add">+</span><span class="diff-line diff-line-add">+Your task:</span><span class="diff-line diff-line-add">+- Provide a clear, direct, and precise answer to the main query using the information from all sub-question answers.</span><span class="diff-line diff-line-add">+- Combine all relevant details logically, avoiding repetition.</span><span class="diff-line diff-line-add">+- If comparing solutions, cite performance differences, cost comparisons, or deployment timelines.</span><span class="diff-line diff-line-add">+- Extract and emphasize **numerical, statistical, or measurable insights** (e.g., costs, quantities, throughput, failure rates, timeframes, benchmarks)</span><span class="diff-line diff-line-add">+- Highlight key technical points and quantitative facts (numbers, metrics, dates, statistics) wherever possible.</span><span class="diff-line diff-line-add">+- Use professional, technical language suitable for engineers or domain experts.</span><span class="diff-line diff-line-add">+- Structure the answer with clear sections or bullet points if it improves readability.</span><span class="diff-line diff-line-add">+- If some sub-questions lack enough info, either skip them or briefly mention that more data is needed.</span><span class="diff-line diff-line-add">+- Do not write a full report; focus on answering the query in a helpful, actionable way.</span><span class="diff-line diff-line-add">+</span><span class="diff-line diff-line-add">+** Citation Guidelines</span><span class="diff-line diff-line-add">+- Use [n] style citations pointing to the correct numbered source</span><span class="diff-line diff-line-add">+- Write the final summary using inline citations like [1], [2], etc.</span><span class="diff-line diff-line-add">+- Do not convert citations or references such as [1] into footnotes in markdown format i.e. [^1].</span><span class="diff-line diff-line-add">+- When referencing information from a source, cite the appropriate source(s) using their corresponding numbers. Only cite a source when you are explicitly referencing it.</span><span class="diff-line diff-line-add">+- Ensure that while citing multiple sources, enclose those citations within brackets separately such as [1][3]. Do not cite sources like this [1,3]. </span><span class="diff-line diff-line-add">+</span><span class="diff-line diff-line-add">+Main Query:  </span><span class="diff-line diff-line-add">+&lt;|QUERY|&gt;:{query}</span><span class="diff-line diff-line-add">+</span><span class="diff-line diff-line-add">+Sources:</span><span class="diff-line diff-line-add">+{sources_text}</span><span class="diff-line diff-line-add">+</span><span class="diff-line diff-line-add">+Subquery Answers text:</span><span class="diff-line diff-line-add">+{subquery_answers_text}</span><span class="diff-line diff-line-add">+</span><span class="diff-line diff-line-add">+</span><span class="diff-line diff-line-add">+Output the synthesized answer only, in markdown format if appropriate, Aim for **500-1000 words**, Be exact, thorough, and evidence-based..</span><span class="diff-line diff-line-add">+</span><span class="diff-line diff-line-add">+&lt;|OUTPUT|&gt;:</span></div></div><div id="file-core-retrieval_api-prompts-report_summarizer-prompt" data-filepath="core/retrieval_api/prompts/report_summarizer.prompt" class="diff-file-section rounded-3xl border border-[#30363d] overflow-hidden mb-8 bg-[#0d1117]">
                        <div class="diff-header">
                            <div class="flex space-x-2">
                                <div class="w-3 h-3 rounded-full bg-[#fa7970]"></div>
                                <div class="w-3 h-3 rounded-full bg-[#faa356]"></div>
                                <div class="w-3 h-3 rounded-full bg-[#7ce38b]"></div>
                            </div>
                            <span class="ml-2 font-mono text-xs opacity-70">core/retrieval_api/prompts/report_summarizer.prompt</span>
                        </div>
                        <div class="code-body font-mono text-sm py-2">
                    <span class="diff-line diff-line-meta">index 82a7ceb..1d6c432 100644</span><span class="diff-line diff-line-meta">--- a/core/retrieval_api/prompts/report_summarizer.prompt</span><span class="diff-line diff-line-meta">+++ b/core/retrieval_api/prompts/report_summarizer.prompt</span><span class="diff-line diff-line-meta text-[#8b949e]">@@ -1,55 +1,52 @@</span><span class="diff-line text-[#c9d1d9]"> You are an expert technical report writer with domain knowledge in the oil and gas industry.</span><span class="diff-line text-[#c9d1d9]"> </span><span class="diff-line text-[#c9d1d9]"> You are given:</span><span class="diff-line diff-line-add">+- User's main question</span><span class="diff-line text-[#c9d1d9]"> - Answers to smaller subquestions (below)</span><span class="diff-line text-[#c9d1d9]"> - A source list with citation numbers</span><span class="diff-line text-[#c9d1d9]"> </span><span class="diff-line diff-line-del">-Your job is to generate a coherent,detailed, relevant,highly technical and factually accurate answer in MLA 9th edition style format. </span><span class="diff-line diff-line-del">-for the given main question, Ensure that the answer is provided in a conversational tone.</span><span class="diff-line diff-line-add">+Your output must be a **unified, deeply analytical research report** to the main research question, synthesized from the sub-question and first pass answers.</span><span class="diff-line text-[#c9d1d9]"> </span><span class="diff-line text-[#c9d1d9]"> Your job is to:</span><span class="diff-line diff-line-del">-- Synthesize a single coherent answer</span><span class="diff-line diff-line-del">-- Use [n] style citations pointing to the correct numbered source</span><span class="diff-line diff-line-del">-- Write the final summary using inline citations like [1], [2], etc.</span><span class="diff-line diff-line-add">+- Synthesize a single coherently written report.</span><span class="diff-line diff-line-add">+</span><span class="diff-line diff-line-add">+</span><span class="diff-line text-[#c9d1d9]"> </span><span class="diff-line diff-line-del">-**The report must:**</span><span class="diff-line diff-line-del">-- Directly and comprehensively answer the main research question</span><span class="diff-line diff-line-del">-- Organize the report naturally into logical sections based on content  do not use a fixed template.</span><span class="diff-line diff-line-del">-- Use all the information provided in the sub-question answers</span><span class="diff-line diff-line-add">+** Content Guidelines</span><span class="diff-line diff-line-add">+- Use all the information provided in the sub-question answers, relevant to the main question.</span><span class="diff-line diff-line-add">+- If comparing solutions, cite performance differences, cost comparisons, or deployment timelines.</span><span class="diff-line text-[#c9d1d9]"> - Extract and emphasize **numerical, statistical, or measurable insights** (e.g., costs, quantities, throughput, failure rates, timeframes, benchmarks)</span><span class="diff-line diff-line-del">-- Be written for a **technical audience** such as engineers, researchers, or decision-makers</span><span class="diff-line diff-line-add">+- Avoid generalizations  all points should be data-backed and precise.</span><span class="diff-line diff-line-add">+- **If a sub-question lacks sufficient information to provide a detailed answer:**Do not write Not applicable.Instead, state that further data or research is needed, or gracefully omit the section</span><span class="diff-line diff-line-add">+- Answer must not contain words such as `Based on the provided sources` and `In this context`</span><span class="diff-line diff-line-add">+- If none of the sources are helpful, mention that you do not have any data that answers the question and answer the user to add more details to the question.</span><span class="diff-line diff-line-add">+- Ensure that the answer is provided in a professional tone.</span><span class="diff-line diff-line-add">+</span><span class="diff-line diff-line-add">+** Structure Guidelines</span><span class="diff-line diff-line-add">+- Begin with a concise executive summary (1 or 2 paragraph) highlighting key insights.</span><span class="diff-line diff-line-add">+- The report must be comprehensive and structured, flowing logically through technical sections  no fixed template, structure should follow the content.</span><span class="diff-line diff-line-add">+- If relevant include Final Summary or Recommendation Section</span><span class="diff-line diff-line-add">+- Highlight feasibility, next steps, or critical trade-offs</span><span class="diff-line diff-line-add">+- **Data Tables**- Use tables to summarize key comparisons, cost ranges, or technical specs</span><span class="diff-line diff-line-add">+- Report length: Aim for **25004000 words** depending on input length and complexity</span><span class="diff-line diff-line-add">+- Report must be detailed.</span><span class="diff-line diff-line-add">+</span><span class="diff-line diff-line-add">+** Formatting Guidelines.</span><span class="diff-line diff-line-add">+- answer should be generated in MLA 9th edition style format.</span><span class="diff-line diff-line-add">+** Use markdown syntax (`##`, `###`, `-`) for structure and readability.</span><span class="diff-line text-[#c9d1d9]"> - Present insights in a clear and organized structure using **markdown formatting**, including:</span><span class="diff-line text-[#c9d1d9]">   - Headings and subheadings</span><span class="diff-line text-[#c9d1d9]">   - Numbered sections</span><span class="diff-line text-[#c9d1d9]">   - Bullet points</span><span class="diff-line text-[#c9d1d9]">   - Tables (if helpful)</span><span class="diff-line diff-line-del">-- Avoid generic or vague summaries  provide **precise, specific, and technical language** with **deep analysis**.</span><span class="diff-line diff-line-del">-- Maintain a professional tone.</span><span class="diff-line diff-line-del">-- **If a sub-question lacks sufficient information to provide a detailed answer:**Do not write Not applicable.Instead, state that further data or research is needed, or gracefully omit the section</span><span class="diff-line diff-line-del">-- Answer must not contain words such as `Based on the provided sources` and `In this context`</span><span class="diff-line diff-line-del">-- Do not convert citations or references such as [1] into footnotes in markdown format i.e. [^1].</span><span class="diff-line diff-line-del">-When referencing information from a source, cite the appropriate source(s) using their corresponding numbers. Only cite a source when you are explicitly referencing it.</span><span class="diff-line diff-line-del">-Ensure that while citing multiple sources, enclose those citations within brackets separately such as [1][3]. Do not cite sources like this [1,3]. </span><span class="diff-line diff-line-del">-If none of the sources are helpful, mention that you do not have any data that answers the question and answer the user to add more details to the question.</span><span class="diff-line diff-line-del">-</span><span class="diff-line text-[#c9d1d9]"> </span><span class="diff-line text-[#c9d1d9]"> </span><span class="diff-line diff-line-del">-**Document structure Guidelines:**</span><span class="diff-line diff-line-del">-Use markdown syntax (`##`, `###`, `-`) for structure and readability.</span><span class="diff-line diff-line-del">-(Briefly summarize the key insights and findings.)</span><span class="diff-line diff-line-del">-(*Extract as many relevant numbers as possible.*)</span><span class="diff-line diff-line-del">-(*Get as many sub-topics as possible*)</span><span class="diff-line diff-line-del">-Eg:</span><span class="diff-line diff-line-del">-- Sub-topic A</span><span class="diff-line diff-line-del">-(Details, data points, techniques, diagrams if applicable)</span><span class="diff-line diff-line-del">-- Sub-topic B</span><span class="diff-line diff-line-del">-(Details with numeric evidence)</span><span class="diff-line diff-line-meta">-----</span><span class="diff-line diff-line-del">-(Feasibility, scalability, environmental or safety concerns)</span><span class="diff-line diff-line-del">-(If available in the input, compare approaches or show tested results.)</span><span class="diff-line diff-line-del">-(Summarize the implications, feasibility, or next steps.)</span><span class="diff-line diff-line-del">-</span><span class="diff-line diff-line-meta">----</span><span class="diff-line diff-line-add">+** Citation Guidelines</span><span class="diff-line diff-line-add">+- Use [n] style citations pointing to the correct numbered source</span><span class="diff-line diff-line-add">+- Write the final summary using inline citations like [1], [2], etc.</span><span class="diff-line diff-line-add">+- Do not convert citations or references such as [1] into footnotes in markdown format i.e. [^1].</span><span class="diff-line diff-line-add">+- When referencing information from a source, cite the appropriate source(s) using their corresponding numbers. Only cite a source when you are explicitly referencing it.</span><span class="diff-line diff-line-add">+- Ensure that while citing multiple sources, enclose those citations within brackets separately such as [1][3]. Do not cite sources like this [1,3]. </span><span class="diff-line text-[#c9d1d9]"> </span><span class="diff-line text-[#c9d1d9]"> Main Research Question:</span><span class="diff-line text-[#c9d1d9]"> &lt;|QUERY|&gt;:{query}</span><span class="diff-line diff-line-meta text-[#8b949e]">@@ -60,33 +57,6 @@ Sources:</span><span class="diff-line text-[#c9d1d9]"> Subquery Answers text:</span><span class="diff-line text-[#c9d1d9]"> {subquery_answers_text}</span><span class="diff-line text-[#c9d1d9]"> </span><span class="diff-line diff-line-del">-**Output the report in markdown format only. Aim for &lt; 3000 words. Be exact, thorough, and evidence-based.**</span><span class="diff-line text-[#c9d1d9]"> &lt;|OUTPUT|&gt;:</span><span class="diff-line text-[#c9d1d9]"> </span><span class="diff-line text-[#c9d1d9]"> </span><span class="diff-line diff-line-del">-Here are few examples:</span><span class="diff-line diff-line-del">-## Example 1:</span><span class="diff-line diff-line-meta">-------</span><span class="diff-line diff-line-del">-Source 1:</span><span class="diff-line diff-line-del">-In a world where magic is real, a rare gemstone known as the **Dragon's Eye** is discovered to possess immense power.</span><span class="diff-line diff-line-del">-Source 2:</span><span class="diff-line diff-line-del">-The Dragon's Eye is sought after by various factions, including a secretive organization called the *Crimson Order*.</span><span class="diff-line diff-line-del">-Source 3:</span><span class="diff-line diff-line-del">-A young adventurer named Aria embarks on a quest to find the Dragon's Eye before it falls into the wrong hands.</span><span class="diff-line diff-line-meta">-------</span><span class="diff-line diff-line-del">-&lt;|QUERY|&gt;: What is the significance of the Dragon's Eye, and why is Aria's quest to find it so important?</span><span class="diff-line diff-line-del">-&lt;|ANSWER|&gt;: `Aria's quest` to find the immensely powerful **Dragon's Eye** [1] is crucial to prevent the secretive *Crimson Order* [2], and other factions from obtaining the rare gemstone and potentially misusing its magic [3].</span><span class="diff-line diff-line-del">-</span><span class="diff-line diff-line-del">-## Example 2:</span><span class="diff-line diff-line-meta">-------</span><span class="diff-line diff-line-del">-Source 1:</span><span class="diff-line diff-line-del">-The housing market in Bangalore has seen a significant rise in prices, with an average increase of 10% over the past year.</span><span class="diff-line diff-line-del">-Source 2:</span><span class="diff-line diff-line-del">-Many residents of Bangalore are concerned about the rising cost of living, which includes the increasing property prices in prime areas.</span><span class="diff-line diff-line-del">-Source 3:</span><span class="diff-line diff-line-del">-Bangalore's real estate market is expected to continue growing, driven by the tech industry and the influx of professionals.</span><span class="diff-line diff-line-meta">-------</span><span class="diff-line diff-line-del">-&lt;|QUERY|&gt;: What is Meditation?</span><span class="diff-line diff-line-del">-&lt;|ANSWER|&gt;: **Meditation** is a practice where an individual focuses their mind on a particular object, thought, or activity to achieve *mental clarity* and *emotional calmness*. It often involves\n-deep breathing\n-mindfulness techniques to enhance concentration and reduce stress.</span><span class="diff-line diff-line-del">-</span><span class="diff-line diff-line-del">-</span></div></div><div id="file-core-retrieval_api-prompts-subquery-prompt" data-filepath="core/retrieval_api/prompts/subquery.prompt" class="diff-file-section rounded-3xl border border-[#30363d] overflow-hidden mb-8 bg-[#0d1117] active-focus">
                        <div class="diff-header">
                            <div class="flex space-x-2">
                                <div class="w-3 h-3 rounded-full bg-[#fa7970]"></div>
                                <div class="w-3 h-3 rounded-full bg-[#faa356]"></div>
                                <div class="w-3 h-3 rounded-full bg-[#7ce38b]"></div>
                            </div>
                            <span class="ml-2 font-mono text-xs opacity-70">core/retrieval_api/prompts/subquery.prompt</span>
                        </div>
                        <div class="code-body font-mono text-sm py-2">
                    <span class="diff-line diff-line-meta">index 021b790..c3963f7 100644</span><span class="diff-line diff-line-meta">--- a/core/retrieval_api/prompts/subquery.prompt</span><span class="diff-line diff-line-meta">+++ b/core/retrieval_api/prompts/subquery.prompt</span><span class="diff-line diff-line-meta text-[#8b949e]">@@ -1,23 +1,60 @@</span><span class="diff-line diff-line-del">-You are an expert at generating follow-up questions to deepen understanding and gather more detailed information based on an initial answer to a user's query.</span><span class="diff-line diff-line-del">-Given a high-level research question and first pass answer, your task is to decompose it into 6 to 8 focused and technically detailed **technically rich**, **measurable**, and **data-driven** components or sub-questions that collectively form a roadmap or outline for a comprehensive, data-rich research report.</span><span class="diff-line diff-line-add">+You are a deep research assistant tasked with helping a user explore a complex or research question in the oil and gas domain.</span><span class="diff-line diff-line-add">+Given a high-level research question and first pass retrieval answer, </span><span class="diff-line diff-line-add">+your task is to break down the users main research question into multiple **relevant sub-questions**, that collectively form a roadmap or outline for a comprehensive, data-rich research report on the main question..</span><span class="diff-line text-[#c9d1d9]"> </span><span class="diff-line text-[#c9d1d9]"> &lt;|QUERY|&gt;: {query}</span><span class="diff-line text-[#c9d1d9]"> &lt;|FIRSTPASSANSWER|&gt;: {first_pass_answer}</span><span class="diff-line text-[#c9d1d9]"> </span><span class="diff-line diff-line-del">-Based on the user's original query and the initial RAG answer, generate a list of **6-8 specific and distinct sub-queries** that would help to:</span><span class="diff-line diff-line-add">+Based on the user's main research question and the initial RAG answer, generate a list of **10-15 specific and distinct sub-queries** that would help to:</span><span class="diff-line text-[#c9d1d9]"> </span><span class="diff-line diff-line-del">-* **Elaborate on key points** mentioned in the initial answer.</span><span class="diff-line diff-line-del">-* **Explore related aspects** that were touched upon but not fully explained or that logically extend from the initial information, even if not directly stated.</span><span class="diff-line diff-line-del">-* **Identify potential ambiguities or unanswered questions** arising from the initial answer.</span><span class="diff-line diff-line-del">-* **Seek more granular details** on specific entities, concepts, or processes mentioned.</span><span class="diff-line diff-line-del">-* **Require **quantified answers** (e.g., KPIs, efficiency %, cost per barrel, downtime, throughput, emissions, ROI)</span><span class="diff-line diff-line-add">+* ** Broaden the investigative scope across technical, historical, economic, regulatory, ethical, environmental, and geopolitical dimensions.</span><span class="diff-line diff-line-add">+* ** Maintain clear relevance to the main research question and topic and original research goal.</span><span class="diff-line diff-line-add">+* ** Generate sub-queries, that meaningfully expand the research.</span><span class="diff-line diff-line-add">+* ** Seek **more granular insights** into specific entities, processes, technologies, or stakeholders mentioned in the main question and first-pass answer.</span><span class="diff-line diff-line-add">+* ** **Uncover underlying mechanisms, causes, or effects** that explain or extend the first-pass insights or initial question.</span><span class="diff-line text-[#c9d1d9]"> * ** Be specific, measurable, and relevant to the oil and gas context</span><span class="diff-line diff-line-del">-* ** Be worded in a way that encourages **quantitative and evidence-based answers**</span><span class="diff-line diff-line-del">-* **Identify potential underlying mechanisms, causes, or effects** related to the initial answer.</span><span class="diff-line diff-line-del">-* **Uncover different perspectives or contrasting viewpoints** on the topics discussed.</span><span class="diff-line diff-line-del">-* **Investigate practical applications, real-world examples, or further developments** connected to the initial information.</span><span class="diff-line diff-line-del">-* ** only generate list of subqueries do not include any intent or any other text apart from them.</span><span class="diff-line diff-line-del">-Ensure that the sub-queries are phrased as clear and concise questions that can be used for further information retrieval. Avoid generating overly broad or redundant questions.</span><span class="diff-line diff-line-del">-* ** only generate list of subqueries DO NOT include any intent or any other text apart from them</span><span class="diff-line diff-line-add">+* ** Avoid repeating prior sub-questions or restating information already covered.</span><span class="diff-line diff-line-add">+* ** if it is relevant to the main question and topic, you can generate sub-questions about practical applications, real-world examples, or further developments** connected to the initial information.</span><span class="diff-line diff-line-add">+**Ensure comprehensive coverage** of the original question by exploring **missing dimensions** (technical, economic, ESG, geopolitical, etc.) if the missing dimesntions is relevant to the main question's research topic.</span><span class="diff-line diff-line-add">+* ** sub question should aim to make the research focused and technically detailed **technically rich**, **measurable**, and **data-driven** +</span><span class="diff-line diff-line-add">+## Guidelines:</span><span class="diff-line diff-line-add">+- Stay grounded in retrieved context; avoid hallucination.</span><span class="diff-line diff-line-add">+- If no relevant docs are found, indicate that and suggest a refinement.</span><span class="diff-line diff-line-add">+- Maintain clarity, depth, and domain specificity.</span><span class="diff-line diff-line-add">+- only generate json array of subqueries. DO NOT include any other text apart from sub_queries.if you can not divide into sub-questions just return original question string in json array.</span><span class="diff-line diff-line-add">+- only generate list of subqueries do not include any intent or any other text apart from them.</span><span class="diff-line diff-line-add">+- Ensure that the sub-queries are phrased as clear and concise questions. </span><span class="diff-line diff-line-add">+- Avoid generating overly broad or redundant questions.</span><span class="diff-line diff-line-add">+</span><span class="diff-line diff-line-add">+Only return output in this format:</span><span class="diff-line diff-line-add">+[</span><span class="diff-line diff-line-add">+  "Sub-question 1?",</span><span class="diff-line diff-line-add">+  "Sub-question 2?",</span><span class="diff-line diff-line-add">+  "Sub-question 3?",</span><span class="diff-line diff-line-add">+  ...</span><span class="diff-line diff-line-add">+]</span><span class="diff-line diff-line-add">+</span><span class="diff-line diff-line-add">+Here is an example:</span><span class="diff-line diff-line-add">+&lt;|QUERY|&gt;: What are the technical, economic, and environmental impacts of transitioning offshore oil platforms to integrate carbon capture and storage (CCS) technologies?</span><span class="diff-line diff-line-add">+&lt;|SUBQUERIES|&gt;:</span><span class="diff-line diff-line-add">+   [</span><span class="diff-line diff-line-add">+    "What are the current technologies available for integrating carbon capture and storage on existing offshore oil platforms?",</span><span class="diff-line diff-line-add">+    "What modifications are necessary for offshore platforms to support CCS equipment and operations?",</span><span class="diff-line diff-line-add">+    "What is the estimated capital and operational expenditure (CAPEX and OPEX) for CCS retrofitting on offshore platforms?",</span><span class="diff-line diff-line-add">+    "How much CO can be realistically captured and stored per year by retrofitted offshore platforms?",</span><span class="diff-line diff-line-add">+    "What material degradation or safety risks are introduced by CCS operations in offshore environments?",</span><span class="diff-line diff-line-add">+    "How do international and regional regulations affect the deployment of CCS on offshore oil platforms?",</span><span class="diff-line diff-line-add">+    "What are the potential environmental risks associated with CCS leakage or storage failure in offshore settings?",</span><span class="diff-line diff-line-add">+    "What government subsidies, carbon credits, or tax incentives are available to support CCS adoption offshore?",</span><span class="diff-line diff-line-add">+    "How can CCS operations be integrated with existing offshore oil production workflows without significant downtime?",</span><span class="diff-line diff-line-add">+    "What successful examples or pilot projects exist for offshore CCS, and what key lessons have been identified?"</span><span class="diff-line diff-line-add">+  ]</span><span class="diff-line diff-line-add">+</span><span class="diff-line diff-line-add">+</span><span class="diff-line diff-line-add">+Now its your turn</span><span class="diff-line diff-line-add">+* ** only generate json  of subqueries DO NOT include any intent or any other text apart from them</span><span class="diff-line diff-line-add">+</span><span class="diff-line text-[#c9d1d9]"> &lt;|SUBQUERIES|&gt;:</span><span class="diff-line text-[#c9d1d9]"> </span><span class="diff-line diff-line-add">+</span></div></div></div>
        </div>
    </main>

    <!-- Data Injection -->
    <script>
        const PR_RAW_DIFF = `diff --git a/core/retrieval_api/generate.py b/core/retrieval_api/generate.py
index 6dc7415..daac475 100644
--- a/core/retrieval_api/generate.py
+++ b/core/retrieval_api/generate.py
@@ -605,21 +605,57 @@ def _retrieve_documents(self):
      def _has_valid_docs(self, docs):
          return docs and max([doc.score for doc in docs]) > self._config("RAG_SIMILARITY_CUTOFF")
  
+    def _decompose_query(self, query: str) -> List[str]:
+        try:
+            chat_sub_query_prompt = self._prompts.chat_subquery.format(query =query)
+            response = self._model_manager.llm.complete(chat_sub_query_prompt).text.strip()
+            subqueries = self._parse_subqueries(response)
+            if isinstance(subqueries, list) and all(isinstance(q, str) for q in subqueries):
+                return subqueries
+            else:
+                raise ValueError("Invalid format: not a list of strings")
+        except json.JSONDecodeError as e:
+            raise ValueError(f"Subquery response not valid JSON: {e}")
+
      def get_subqueries(self, main_query: str, first_pass_answer:str):
-        logger.info("getting sub queries")
-        response = self._model_manager.llm.complete(
-                self._prompts.subquery.format(query=main_query,first_pass_answer=first_pass_answer)
-            ).text.strip()
-        logger.info("getting sub queries")
-        # Parse sub-queries from numbered output
-        lines = response.strip().split("\\n")
-        subqueries = []
-        for line in lines:
-            if "." in line:
-                subqueries.append(line.split(".", 1)[1].strip())
+        try:
+            logger.info("getting sub queries")
+            response = self._model_manager.llm.complete(
+                    self._prompts.subquery.format(query=main_query,first_pass_answer=first_pass_answer)
+                ).text.strip()
+            subqueries = self._parse_subqueries(response)
+            logger.info("subqueries, {subqueries}")
+            if isinstance(subqueries, list) and all(isinstance(q, str) for q in subqueries):
+                    return subqueries
             else:
-                subqueries.append(line.strip())
-        return subqueries
+                raise ValueError("Invalid format: not a list of strings")
+        except json.JSONDecodeError as e:
+            raise ValueError(f"Subquery response not valid JSON: {e}")
+        # # Parse sub-queries from numbered output
+        # lines = response.strip().split("\\n")
+        # subqueries = []
+        # for line in lines:
+        #     if "." in line:
+        #         subqueries.append(line.split(".", 1)[1].strip())
+        #     else:
+        #         subqueries.append(line.strip())
+        # return subqueries
+
+    # def get_subqueries(self, main_query: str, first_pass_answer:str):
+    #     logger.info("getting sub queries")
+    #     response = self._model_manager.llm.complete(
+    #             self._prompts.subquery.format(query=main_query,first_pass_answer=first_pass_answer)
+    #         ).text.strip()
+    #     logger.info("getting sub queries")
+    #     # Parse sub-queries from numbered output
+    #     lines = response.strip().split("\\n")
+    #     subqueries = []
+    #     for line in lines:
+    #         if "." in line:
+    #             subqueries.append(line.split(".", 1)[1].strip())
+    #         else:
+    #             subqueries.append(line.strip())
+    #     return subqueries
      
      def _handle_compound_query_response(self):
          query_b = QueryBundle(query_str=self._refined_query)
@@ -687,16 +723,20 @@ def _yield_response(self, response):
              if chunk != "Empty Response":
                  answer += chunk
              yield chunk_json
-    
+
      def _handle_compound_query(self):
          query_b = QueryBundle(query_str=self._refined_query)
          sub_queries = self._decompose_query(query_b)
          if sub_queries:
              logger.info(f"sub_queries, {sub_queries}")
-            subq_answers, retrieved_docs, subq_source_nodes = self._handle_sub_queries(sub_queries)
+            subq_answers, retrieved_docs, subq_source_nodes = self._handle_sub_queries_v2(sub_queries)
+            global_citation_map = self._build_citationmap(retrieved_docs)
+            updated_subq_answers=self._process_sub_queries(sub_queries,subq_answers,subq_source_nodes,global_citation_map)
+            sources_text = self._inputs_for_process_cntxt(global_citation_map)
              logger.info(f"final_answer, {subq_answers}")
              # response = self._chat_synthesize_response(query_b, subq_answer)
-            response = self._chat_synthesize_subanswers(query_b, subq_answers)
+            response = self._chat_synthesize_subanswers_v2(self._refined_query, updated_subq_answers, sources_text)
+            # response = self._chat_synthesize_subanswers(query_b, subq_answers)
              answer = ""
  
              for chunk_json in self.response_generator(response.text):
@@ -846,29 +886,29 @@ def _inputs_for_process_cntxt(self,citation_map):
          )
          return sources_text
  
-    def _chat_synthesize_response(self, main_qb:str, subquery_results:List):
+    # def _chat_synthesize_response(self, main_qb:str, subquery_results:List):
  
-        chat_synthesize_prompt = self._prompts.chat_synthesizer.format(query=main_qb,combined_text = subquery_results)
-        response_synth = get_response_synthesizer(
-            response_mode="compact",
-            text_qa_template=chat_synthesize_prompt,
-            llm=self._model_manager.llm
-        )
+    #     chat_synthesize_prompt = self._prompts.chat_synthesizer.format(query=main_qb,combined_text = subquery_results)
+    #     response_synth = get_response_synthesizer(
+    #         response_mode="compact",
+    #         text_qa_template=chat_synthesize_prompt,
+    #         llm=self._model_manager.llm
+    #     )
  
-        subq_nodes = [NodeWithScore(node=TextNode(text=chunk)) for chunk in subquery_results]
-        # subq_nodes = [TextNode(text=chunk) for chunk in subquery_results]
+    #     subq_nodes = [NodeWithScore(node=TextNode(text=chunk)) for chunk in subquery_results]
+    #     # subq_nodes = [TextNode(text=chunk) for chunk in subquery_results]
  
-        response = response_synth.synthesize(main_qb, nodes=subq_nodes)
-        # response = self._model_manager.llm.complete(
-        #         self._prompts.report_summarizer.format(query=main_query,combined_text = subquery_results)
-        #     )
-        return response
+    #     response = response_synth.synthesize(main_qb, nodes=subq_nodes)
+    #     # response = self._model_manager.llm.complete(
+    #     #         self._prompts.report_summarizer.format(query=main_query,combined_text = subquery_results)
+    #     #     )
+    #     return response
      
-    def synthesize_markdown_report(self, main_query:str, subquery_results:List):
-        response = self._model_manager.llm.complete(
-                self._prompts.report_summarizer.format(query=main_query, combined_text = subquery_results)
-            )
-        return response
+    # def synthesize_markdown_report(self, main_query:str, subquery_results:List):
+    #     response = self._model_manager.llm.complete(
+    #             self._prompts.report_summarizer.format(query=main_query, combined_text = subquery_results)
+    #         )
+    #     return response
      
      def _synthesize_markdown_report_v2(self, main_query:str, subquery_answers:Dict, sources_text:List):
          logger.info(f"synthesizing report")
@@ -887,6 +927,20 @@ def _chat_synthesize_subanswers(self, main_query:str, subquery_results:List):
              )
          return response
      
+    def _chat_synthesize_subanswers_v2(self, main_query:str, subquery_answers:Dict,sources_text:List):
+        logger.info(f"synthesizing report")
+        subquery_answers_text = "\\n\\n".join(
+            [f"Subquery {i+1}: {subquery}\\nAnswer: {subquery_answers[subquery]}" 
+            for i, subquery in enumerate(subquery_answers)]
+        )
+        response = self._model_manager.llm.complete(
+                self._prompts.chat_synthesizer_v2.format(query=main_query,sources_text = sources_text, subquery_answers_text = subquery_answers_text)
+            )
+        # response = self._model_manager.llm.complete(
+        #         self._prompts.chat_synthesizer.format(query=main_query,subquery_results = subquery_results)
+        #     )
+        return response
+    
      def response_generator(self, response_str, chunk_size=50):
          for i in range(0, len(response_str), chunk_size):
              chunk = response_str[i : i + chunk_size]
@@ -1069,41 +1123,3 @@ def _generate_conversation_title(self):
                  "type": "title",
                  "text": "Untitled Conversation"
              })
-
-            
-        
-from llama_index.core.schema import QueryBundle
-from llama_index.core.query_engine import CitationQueryEngine
-
-class CompoundQueryHandler:
-    def __init__(self, query_engine: CitationQueryEngine):
-        self.query_engine = query_engine
-
-    def decompose_query(self, query: str) -> list[str]:
-        # Naive logic, replace with LLM if needed
-        if " and " in query:
-            return query.split(" and ")
-        elif " then " in query:
-            return query.split(" then ")
-        else:
-            return [query]
-
-    def handle_query(self, query: str):
-        sub_queries = self.decompose_query(query)
-        all_docs = []
-        all_responses = []
-
-        for sub_query in sub_queries:
-            sub_query = sub_query.strip()
-            query_bundle = QueryBundle(sub_query)
-
-            # Step 1: Retrieve relevant nodes
-            retrieved_docs = self.query_engine.retrieve(query_bundle)
-            all_docs.extend(retrieved_docs)
-
-            # Step 2: Synthesize the response
-            response = self.query_engine.synthesize(query_bundle, retrieved_docs)
-            all_responses.append(f"**Q:** {sub_query}\\n**A:** {str(response)}")
-
-        final_response = "\\n\\n".join(all_responses)
-        return final_response, all_docs
diff --git a/core/retrieval_api/managers/prompt_manager.py b/core/retrieval_api/managers/prompt_manager.py
index ed8c09b..ed5b24c 100644
--- a/core/retrieval_api/managers/prompt_manager.py
+++ b/core/retrieval_api/managers/prompt_manager.py
@@ -22,6 +22,7 @@ class PromptConfig(BaseModel):
      report_summarizer: str = "report_summarizer.prompt"
      subquery: str = "subquery.prompt"
      chat_synthesizer: str = "chat_synthesizer.prompt"
+    chat_synthesizer_v2: str = "chat_synthesizer_v2.prompt"
      chat_subquery:str = "chat_subquery.prompt"
      is_compound_prompt:str = "is_compound_query.prompt"
      class Config:
diff --git a/core/retrieval_api/prompts/chat_subquery.prompt b/core/retrieval_api/prompts/chat_subquery.prompt
index e73b0f3..6c5c778 100644
--- a/core/retrieval_api/prompts/chat_subquery.prompt
+++ b/core/retrieval_api/prompts/chat_subquery.prompt
@@ -31,3 +31,4 @@ Only return output in this format:
    ...
  ]
  
+<|SUBQUERIES|>:
\\ No newline at end of file
diff --git a/core/retrieval_api/prompts/chat_synthesizer_v2.prompt b/core/retrieval_api/prompts/chat_synthesizer_v2.prompt
new file mode 100644
index 0000000..ee42d1e
--- /dev/null
+++ b/core/retrieval_api/prompts/chat_synthesizer_v2.prompt
@@ -0,0 +1,33 @@
+You are an expert technical assistant that synthesizes answers to a main question by integrating responses from several focused sub-questions.
+
+Your task:
+- Provide a clear, direct, and precise answer to the main query using the information from all sub-question answers.
+- Combine all relevant details logically, avoiding repetition.
+- If comparing solutions, cite performance differences, cost comparisons, or deployment timelines.
+- Extract and emphasize **numerical, statistical, or measurable insights** (e.g., costs, quantities, throughput, failure rates, timeframes, benchmarks)
+- Highlight key technical points and quantitative facts (numbers, metrics, dates, statistics) wherever possible.
+- Use professional, technical language suitable for engineers or domain experts.
+- Structure the answer with clear sections or bullet points if it improves readability.
+- If some sub-questions lack enough info, either skip them or briefly mention that more data is needed.
+- Do not write a full report; focus on answering the query in a helpful, actionable way.
+
+** Citation Guidelines
+- Use [n] style citations pointing to the correct numbered source
+- Write the final summary using inline citations like [1], [2], etc.
+- Do not convert citations or references such as [1] into footnotes in markdown format i.e. [^1].
+- When referencing information from a source, cite the appropriate source(s) using their corresponding numbers. Only cite a source when you are explicitly referencing it.
+- Ensure that while citing multiple sources, enclose those citations within brackets separately such as [1][3]. Do not cite sources like this [1,3]. 
+
+Main Query:  
+<|QUERY|>:{query}
+
+Sources:
+{sources_text}
+
+Subquery Answers text:
+{subquery_answers_text}
+
+
+Output the synthesized answer only, in markdown format if appropriate, Aim for **500-1000 words**, Be exact, thorough, and evidence-based..
+
+<|OUTPUT|>:
diff --git a/core/retrieval_api/prompts/report_summarizer.prompt b/core/retrieval_api/prompts/report_summarizer.prompt
index 82a7ceb..1d6c432 100644
--- a/core/retrieval_api/prompts/report_summarizer.prompt
+++ b/core/retrieval_api/prompts/report_summarizer.prompt
@@ -1,55 +1,52 @@
 You are an expert technical report writer with domain knowledge in the oil and gas industry.
 
 You are given:
+- User's main question
 - Answers to smaller subquestions (below)
 - A source list with citation numbers
 
-Your job is to generate a coherent,detailed, relevant,highly technical and factually accurate answer in MLA 9th edition style format. 
-for the given main question, Ensure that the answer is provided in a conversational tone.
+Your output must be a **unified, deeply analytical research report** to the main research question, synthesized from the sub-question and first pass answers.
 
 Your job is to:
-- Synthesize a single coherent answer
-- Use [n] style citations pointing to the correct numbered source
-- Write the final summary using inline citations like [1], [2], etc.
+- Synthesize a single coherently written report.
+
+
 
-**The report must:**
-- Directly and comprehensively answer the main research question
-- Organize the report naturally into logical sections based on content  do not use a fixed template.
-- Use all the information provided in the sub-question answers
+** Content Guidelines
+- Use all the information provided in the sub-question answers, relevant to the main question.
+- If comparing solutions, cite performance differences, cost comparisons, or deployment timelines.
 - Extract and emphasize **numerical, statistical, or measurable insights** (e.g., costs, quantities, throughput, failure rates, timeframes, benchmarks)
-- Be written for a **technical audience** such as engineers, researchers, or decision-makers
+- Avoid generalizations  all points should be data-backed and precise.
+- **If a sub-question lacks sufficient information to provide a detailed answer:**Do not write Not applicable.Instead, state that further data or research is needed, or gracefully omit the section
+- Answer must not contain words such as \`Based on the provided sources\` and \`In this context\`
+- If none of the sources are helpful, mention that you do not have any data that answers the question and answer the user to add more details to the question.
+- Ensure that the answer is provided in a professional tone.
+
+** Structure Guidelines
+- Begin with a concise executive summary (1 or 2 paragraph) highlighting key insights.
+- The report must be comprehensive and structured, flowing logically through technical sections  no fixed template, structure should follow the content.
+- If relevant include Final Summary or Recommendation Section
+- Highlight feasibility, next steps, or critical trade-offs
+- **Data Tables**- Use tables to summarize key comparisons, cost ranges, or technical specs
+- Report length: Aim for **25004000 words** depending on input length and complexity
+- Report must be detailed.
+
+** Formatting Guidelines.
+- answer should be generated in MLA 9th edition style format.
+** Use markdown syntax (\`##\`, \`###\`, \`-\`) for structure and readability.
 - Present insights in a clear and organized structure using **markdown formatting**, including:
   - Headings and subheadings
   - Numbered sections
   - Bullet points
   - Tables (if helpful)
-- Avoid generic or vague summaries  provide **precise, specific, and technical language** with **deep analysis**.
-- Maintain a professional tone.
-- **If a sub-question lacks sufficient information to provide a detailed answer:**Do not write Not applicable.Instead, state that further data or research is needed, or gracefully omit the section
-- Answer must not contain words such as \`Based on the provided sources\` and \`In this context\`
-- Do not convert citations or references such as [1] into footnotes in markdown format i.e. [^1].
-When referencing information from a source, cite the appropriate source(s) using their corresponding numbers. Only cite a source when you are explicitly referencing it.
-Ensure that while citing multiple sources, enclose those citations within brackets separately such as [1][3]. Do not cite sources like this [1,3]. 
-If none of the sources are helpful, mention that you do not have any data that answers the question and answer the user to add more details to the question.
-
 
 
-**Document structure Guidelines:**
-Use markdown syntax (\`##\`, \`###\`, \`-\`) for structure and readability.
-(Briefly summarize the key insights and findings.)
-(*Extract as many relevant numbers as possible.*)
-(*Get as many sub-topics as possible*)
-Eg:
-- Sub-topic A
-(Details, data points, techniques, diagrams if applicable)
-- Sub-topic B
-(Details with numeric evidence)
-----
-(Feasibility, scalability, environmental or safety concerns)
-(If available in the input, compare approaches or show tested results.)
-(Summarize the implications, feasibility, or next steps.)
-
----
+** Citation Guidelines
+- Use [n] style citations pointing to the correct numbered source
+- Write the final summary using inline citations like [1], [2], etc.
+- Do not convert citations or references such as [1] into footnotes in markdown format i.e. [^1].
+- When referencing information from a source, cite the appropriate source(s) using their corresponding numbers. Only cite a source when you are explicitly referencing it.
+- Ensure that while citing multiple sources, enclose those citations within brackets separately such as [1][3]. Do not cite sources like this [1,3]. 
 
 Main Research Question:
 <|QUERY|>:{query}
@@ -60,33 +57,6 @@ Sources:
 Subquery Answers text:
 {subquery_answers_text}
 
-**Output the report in markdown format only. Aim for < 3000 words. Be exact, thorough, and evidence-based.**
 <|OUTPUT|>:
 
 
-Here are few examples:
-## Example 1:
-------
-Source 1:
-In a world where magic is real, a rare gemstone known as the **Dragon's Eye** is discovered to possess immense power.
-Source 2:
-The Dragon's Eye is sought after by various factions, including a secretive organization called the *Crimson Order*.
-Source 3:
-A young adventurer named Aria embarks on a quest to find the Dragon's Eye before it falls into the wrong hands.
-------
-<|QUERY|>: What is the significance of the Dragon's Eye, and why is Aria's quest to find it so important?
-<|ANSWER|>: \`Aria's quest\` to find the immensely powerful **Dragon's Eye** [1] is crucial to prevent the secretive *Crimson Order* [2], and other factions from obtaining the rare gemstone and potentially misusing its magic [3].
-
-## Example 2:
-------
-Source 1:
-The housing market in Bangalore has seen a significant rise in prices, with an average increase of 10% over the past year.
-Source 2:
-Many residents of Bangalore are concerned about the rising cost of living, which includes the increasing property prices in prime areas.
-Source 3:
-Bangalore's real estate market is expected to continue growing, driven by the tech industry and the influx of professionals.
-------
-<|QUERY|>: What is Meditation?
-<|ANSWER|>: **Meditation** is a practice where an individual focuses their mind on a particular object, thought, or activity to achieve *mental clarity* and *emotional calmness*. It often involves\\n-deep breathing\\n-mindfulness techniques to enhance concentration and reduce stress.
-
-
diff --git a/core/retrieval_api/prompts/subquery.prompt b/core/retrieval_api/prompts/subquery.prompt
index 021b790..c3963f7 100644
--- a/core/retrieval_api/prompts/subquery.prompt
+++ b/core/retrieval_api/prompts/subquery.prompt
@@ -1,23 +1,60 @@
-You are an expert at generating follow-up questions to deepen understanding and gather more detailed information based on an initial answer to a user's query.
-Given a high-level research question and first pass answer, your task is to decompose it into 6 to 8 focused and technically detailed **technically rich**, **measurable**, and **data-driven** components or sub-questions that collectively form a roadmap or outline for a comprehensive, data-rich research report.
+You are a deep research assistant tasked with helping a user explore a complex or research question in the oil and gas domain.
+Given a high-level research question and first pass retrieval answer, 
+your task is to break down the users main research question into multiple **relevant sub-questions**, that collectively form a roadmap or outline for a comprehensive, data-rich research report on the main question..
 
 <|QUERY|>: {query}
 <|FIRSTPASSANSWER|>: {first_pass_answer}
 
-Based on the user's original query and the initial RAG answer, generate a list of **6-8 specific and distinct sub-queries** that would help to:
+Based on the user's main research question and the initial RAG answer, generate a list of **10-15 specific and distinct sub-queries** that would help to:
 
-* **Elaborate on key points** mentioned in the initial answer.
-* **Explore related aspects** that were touched upon but not fully explained or that logically extend from the initial information, even if not directly stated.
-* **Identify potential ambiguities or unanswered questions** arising from the initial answer.
-* **Seek more granular details** on specific entities, concepts, or processes mentioned.
-* **Require **quantified answers** (e.g., KPIs, efficiency %, cost per barrel, downtime, throughput, emissions, ROI)
+* ** Broaden the investigative scope across technical, historical, economic, regulatory, ethical, environmental, and geopolitical dimensions.
+* ** Maintain clear relevance to the main research question and topic and original research goal.
+* ** Generate sub-queries, that meaningfully expand the research.
+* ** Seek **more granular insights** into specific entities, processes, technologies, or stakeholders mentioned in the main question and first-pass answer.
+* ** **Uncover underlying mechanisms, causes, or effects** that explain or extend the first-pass insights or initial question.
 * ** Be specific, measurable, and relevant to the oil and gas context
-* ** Be worded in a way that encourages **quantitative and evidence-based answers**
-* **Identify potential underlying mechanisms, causes, or effects** related to the initial answer.
-* **Uncover different perspectives or contrasting viewpoints** on the topics discussed.
-* **Investigate practical applications, real-world examples, or further developments** connected to the initial information.
-* ** only generate list of subqueries do not include any intent or any other text apart from them.
-Ensure that the sub-queries are phrased as clear and concise questions that can be used for further information retrieval. Avoid generating overly broad or redundant questions.
-* ** only generate list of subqueries DO NOT include any intent or any other text apart from them
+* ** Avoid repeating prior sub-questions or restating information already covered.
+* ** if it is relevant to the main question and topic, you can generate sub-questions about practical applications, real-world examples, or further developments** connected to the initial information.
+**Ensure comprehensive coverage** of the original question by exploring **missing dimensions** (technical, economic, ESG, geopolitical, etc.) if the missing dimesntions is relevant to the main question's research topic.
+* ** sub question should aim to make the research focused and technically detailed **technically rich**, **measurable**, and **data-driven** +
+## Guidelines:
+- Stay grounded in retrieved context; avoid hallucination.
+- If no relevant docs are found, indicate that and suggest a refinement.
+- Maintain clarity, depth, and domain specificity.
+- only generate json array of subqueries. DO NOT include any other text apart from sub_queries.if you can not divide into sub-questions just return original question string in json array.
+- only generate list of subqueries do not include any intent or any other text apart from them.
+- Ensure that the sub-queries are phrased as clear and concise questions. 
+- Avoid generating overly broad or redundant questions.
+
+Only return output in this format:
+[
+  "Sub-question 1?",
+  "Sub-question 2?",
+  "Sub-question 3?",
+  ...
+]
+
+Here is an example:
+<|QUERY|>: What are the technical, economic, and environmental impacts of transitioning offshore oil platforms to integrate carbon capture and storage (CCS) technologies?
+<|SUBQUERIES|>:
+   [
+    "What are the current technologies available for integrating carbon capture and storage on existing offshore oil platforms?",
+    "What modifications are necessary for offshore platforms to support CCS equipment and operations?",
+    "What is the estimated capital and operational expenditure (CAPEX and OPEX) for CCS retrofitting on offshore platforms?",
+    "How much CO can be realistically captured and stored per year by retrofitted offshore platforms?",
+    "What material degradation or safety risks are introduced by CCS operations in offshore environments?",
+    "How do international and regional regulations affect the deployment of CCS on offshore oil platforms?",
+    "What are the potential environmental risks associated with CCS leakage or storage failure in offshore settings?",
+    "What government subsidies, carbon credits, or tax incentives are available to support CCS adoption offshore?",
+    "How can CCS operations be integrated with existing offshore oil production workflows without significant downtime?",
+    "What successful examples or pilot projects exist for offshore CCS, and what key lessons have been identified?"
+  ]
+
+
+Now its your turn
+* ** only generate json  of subqueries DO NOT include any intent or any other text apart from them
+
 <|SUBQUERIES|>:
 
+`;

        window.WALKTHROUGH_JSON = [
            {
                "id": 1,
                "title": "Robust JSON Parsing for Sub-query Decomposition",
                "description": "Replaces fragile string splitting logic (splitting by newline/period) with a structured JSON parsing approach. The _decompose_query method now expects a JSON list from the LLM and includes error handling (JSONDecodeError) to ensure downstream processing receives a valid list of strings. This aligns with the prompt changes enforcing JSON output.",
                "file_path": "core/retrieval_api/generate.py",
                "related_component": "RAGGenerator._decompose_query / get_subqueries",
                "change_type": "Logic Fix & Robustness"
            },
            {
                "id": 2,
                "title": "Sub-query Prompt Tuning & JSON Enforcement",
                "description": "Heavily modifies the system prompt to enforce strict JSON array output ([\"Q1\", \"Q2\"]) to support the new parsing logic in generate.py. It also increases the requested sub-query count from 6-8 to 10-15 and adds specific domain context ('Oil and Gas') to drive deeper research.",
                "file_path": "core/retrieval_api/prompts/subquery.prompt",
                "related_component": "System Prompt (Subquery)",
                "change_type": "Prompt Engineering"
            },
            {
                "id": 3,
                "title": "V2 Compound Query Pipeline Integration",
                "description": "Updates the orchestration logic in _handle_compound_query. It transitions to V2 methods (_handle_sub_queries_v2) and introduces a new step to build a global_citation_map and process source text before synthesis. This ensures that the final answer has access to consolidated citation data across all sub-queries.",
                "file_path": "core/retrieval_api/generate.py",
                "related_component": "RAGGenerator._handle_compound_query",
                "change_type": "Refactor & Feature Update"
            },
            {
                "id": 4,
                "title": "New V2 Synthesis Logic",
                "description": "Implements _chat_synthesize_subanswers_v2, which formats the prompt input to include both sources_text (for citations) and subquery_answers_text. This prepares the context for the newly added chat_synthesizer_v2 prompt, replacing the previous simple concatenation strategy.",
                "file_path": "core/retrieval_api/generate.py",
                "related_component": "RAGGenerator._chat_synthesize_subanswers_v2",
                "change_type": "Logic Update"
            },
            {
                "id": 5,
                "title": "New Chat Synthesis Prompt (V2)",
                "description": "Introduces a new prompt template focused on integrating sub-question answers into a coherent response with strict citation guidelines ([n] style) and requirements for numerical/statistical insights. This drives the 'chat compound query response modified' objective.",
                "file_path": "core/retrieval_api/prompts/chat_synthesizer_v2.prompt",
                "related_component": "System Prompt (Chat Synthesizer V2)",
                "change_type": "New Asset"
            },
            {
                "id": 6,
                "title": "Report Summarizer Overhaul",
                "description": "Refactors the report prompt from a conversational style to a 'deeply analytical research report'. It adds specific structure requirements (Executive Summary, Data Tables), increases target word count (2500-4000), and emphasizes 'Oil and Gas' domain knowledge.",
                "file_path": "core/retrieval_api/prompts/report_summarizer.prompt",
                "related_component": "System Prompt (Report Summarizer)",
                "change_type": "Prompt Engineering"
            }
        ];

        let activeStep = 0;

        /* =========================================
           1. DIFF PARSER LOGIC
           ========================================= */
        function parseDiffToHTML(rawDiff) {
            const lines = rawDiff.split('\n');
            let html = '';
            let currentFile = null;
            let fileContainerOpen = false;

            lines.forEach((line, index) => {
                const encodedLine = line.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");
                
                // Detect File Header
                if (line.startsWith('diff --git')) {
                    if (fileContainerOpen) {
                        html += '</div></div>'; // Close previous file container
                    }
                    
                    // Extract filename
                    // Line looks like: diff --git a/path/to/file b/path/to/file
                    const parts = line.split(' ');
                    const bPath = parts[parts.length - 1]; 
                    const cleanPath = bPath.substring(2); // Remove b/

                    // Create ID based on path for scrolling
                    const fileId = `file-${cleanPath.replace(/[\/\.]/g, '-')}`;
                    currentFile = cleanPath;

                    html += `<div id="${fileId}" data-filepath="${cleanPath}" class="diff-file-section rounded-3xl border border-[#30363d] overflow-hidden mb-8 bg-[#0d1117]">`;
                    
                    // Mac-style Window Header
                    html += `
                        <div class="diff-header">
                            <div class="flex space-x-2">
                                <div class="w-3 h-3 rounded-full bg-[#fa7970]"></div>
                                <div class="w-3 h-3 rounded-full bg-[#faa356]"></div>
                                <div class="w-3 h-3 rounded-full bg-[#7ce38b]"></div>
                            </div>
                            <span class="ml-2 font-mono text-xs opacity-70">${cleanPath}</span>
                        </div>
                        <div class="code-body font-mono text-sm py-2">
                    `;
                    fileContainerOpen = true;
                }

                // Skip meta lines (index, ---, +++) if inside a file block to keep it clean, 
                // OR render them as meta. Let's render them as meta for authenticity.
                let lineClass = "diff-line";
                
                if (line.startsWith('diff --git')) {
                   // Already handled above, but we need to print the line inside the code block? 
                   // No, usually diff --git IS the header. Let's skip printing the raw diff --git line in the code block.
                   return; 
                } 
                else if (line.startsWith('index') || line.startsWith('---') || line.startsWith('+++')) {
                    lineClass += " diff-line-meta";
                }
                else if (line.startsWith('@@')) {
                    lineClass += " diff-line-meta text-[#8b949e]";
                }
                else if (line.startsWith('+')) {
                    lineClass += " diff-line-add";
                }
                else if (line.startsWith('-')) {
                    lineClass += " diff-line-del";
                }
                else {
                    lineClass += " text-[#c9d1d9]"; // Default text color
                }

                if (fileContainerOpen) {
                    html += `<span class="${lineClass}">${encodedLine}</span>`;
                }
            });

            if (fileContainerOpen) {
                html += '</div></div>';
            }
            return html;
        }

        /* =========================================
           2. RENDER INITIAL STATE
           ========================================= */
        function init() {
            // Render Diff
            const diffPanel = document.getElementById('diff-content');
            diffPanel.innerHTML = parseDiffToHTML(PR_RAW_DIFF);

            // Render Context Cards
            const contextPanel = document.getElementById('context-panel');
            const cardsHTML = WALKTHROUGH_JSON.map((step, index) => {
                // Determine icon based on change type
                let icon = '';
                if (step.change_type.includes('Fix')) icon = 'text-green-500 bg-green-50';
                else if (step.change_type.includes('Refactor')) icon = 'text-blue-500 bg-blue-50';
                else if (step.change_type.includes('New')) icon = 'text-purple-500 bg-purple-50';
                else icon = 'text-gray-500 bg-gray-50';

                return `
                <div id="step-${index}" class="step-card bg-white p-6 rounded-xl cursor-pointer hover:shadow-md" onclick="activateStep(${index})">
                    <div class="flex justify-between items-start mb-2">
                        <span class="step-number text-xs font-bold text-gray-400 bg-gray-100 px-2 py-1 rounded-full mb-2">#${index + 1}</span>
                        <span class="text-xs font-semibold px-2 py-1 rounded-full ${icon}">${step.change_type}</span>
                    </div>
                    <h3 class="font-bold text-gray-800 text-lg mb-2">${step.title}</h3>
                    <p class="text-gray-600 text-sm leading-relaxed mb-4">${step.description}</p>
                    <div class="flex items-center text-xs text-gray-400 font-mono bg-gray-50 p-2 rounded border border-gray-100">
                        <svg class="w-3 h-3 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg>
                        ${step.related_component}
                    </div>
                </div>
                `;
            }).join('');
            
            // Add spacer at bottom for scrolling
            contextPanel.innerHTML += cardsHTML + '<div class="h-64"></div>';

            document.getElementById('total-steps-display').innerText = WALKTHROUGH_JSON.length;

            // Start
            activateStep(0);
        }

        /* =========================================
           3. INTERACTION LOGIC
           ========================================= */
        window.activateStep = function activateStep(index) {
            activeStep = index;
            const data = WALKTHROUGH_JSON[index];

            // Update UI Counters & Progress
            document.getElementById('current-step-display').innerText = index + 1;
            const progressPct = ((index + 1) / WALKTHROUGH_JSON.length) * 100;
            document.getElementById('progress-bar').style.width = `${progressPct}%`;

            // 1. Highlight Left Card
            document.querySelectorAll('.step-card').forEach((card, idx) => {
                if (idx === index) {
                    card.classList.add('active-card');
                    card.scrollIntoView({ behavior: 'smooth', block: 'center' });
                } else {
                    card.classList.remove('active-card');
                }
            });

            // 2. Diff Lens Logic (Right Panel)
            const targetFilePath = data.file_path;
            const fileBlocks = document.querySelectorAll('.diff-file-section');
            let targetBlock = null;

            fileBlocks.forEach(block => {
                const blockPath = block.getAttribute('data-filepath');
                
                // Reset State
                block.classList.remove('active-focus');
                
                // Simple matching: JSON path must be included in Diff path or vice versa
                // (Handling potential path prefix differences)
                if (blockPath === targetFilePath || blockPath.endsWith(targetFilePath) || targetFilePath.endsWith(blockPath)) {
                    targetBlock = block;
                }
            });

            if (targetBlock) {
                targetBlock.classList.add('active-focus');
                
                // Scroll to the specific file
                // We use a small timeout to allow class transitions to start
                setTimeout(() => {
                    targetBlock.scrollIntoView({ behavior: 'smooth', block: 'center' });
                }, 50);
            } else {
                console.warn("Could not find file block for:", targetFilePath);
            }
        }

        // Initialize
        window.addEventListener('DOMContentLoaded', init);

    </script>

    <!-- Audio-guided walkthrough controller -->
    <script>
        (function () {
            if (!window.WALKTHROUGH_JSON || !window.activateStep) return;

            const steps = window.WALKTHROUGH_JSON;
            const audio = new Audio();
            audio.preload = 'auto';

            let currentIndex = 0;
            let isUserPaused = true;

            function audioSrcFor(index) {
                // step-0.mp3  step-N.mp3 (matches step IDs)
                return `step-${index}.mp3`;
            }

            function loadTrack(index, play = false) {
                currentIndex = Math.max(0, Math.min(steps.length - 1, index));
                audio.src = audioSrcFor(currentIndex);
                try {
                    window.activateStep(currentIndex);
                    // Scroll the active step card into view, centered in viewport
                    const activeCard = document.getElementById(`step-${currentIndex}`);
                    if (activeCard) {
                        activeCard.scrollIntoView({ behavior: 'smooth', block: 'center' });
                    }
                } catch (e) {
                    // ignore if activateStep not ready yet
                }
                if (play) {
                    playCurrent();
                } else {
                    isUserPaused = true;
                    updatePlayIcon();
                }
            }

            function playCurrent() {
                isUserPaused = false;
                audio.play().catch(function () {
                    isUserPaused = true;
                    updatePlayIcon();
                });
                updatePlayIcon();
            }

            function pauseCurrent() {
                isUserPaused = true;
                audio.pause();
                updatePlayIcon();
            }

            function nextTrack() {
                if (currentIndex < steps.length - 1) {
                    loadTrack(currentIndex + 1, !isUserPaused);
                } else {
                    pauseCurrent();
                }
            }

            function prevTrack() {
                if (currentIndex > 0) {
                    loadTrack(currentIndex - 1, !isUserPaused);
                } else {
                    loadTrack(0, !isUserPaused);
                }
            }

            // UI
            const bar = document.createElement('div');
            bar.id = 'pr-audio-controller';
            bar.style.position = 'fixed';
            bar.style.left = '50%';
            bar.style.transform = 'translateX(-50%)';
            bar.style.bottom = '18px';
            bar.style.zIndex = '40';
            bar.style.background = '#0f172a';
            bar.style.color = '#e5e7eb';
            bar.style.borderRadius = '999px';
            bar.style.padding = '8px 18px';
            bar.style.display = 'flex';
            bar.style.alignItems = 'center';
            bar.style.gap = '10px';
            bar.style.boxShadow = '0 12px 30px rgba(15,23,42,0.55)';
            bar.style.fontFamily = 'system-ui, -apple-system, BlinkMacSystemFont, sans-serif';
            bar.innerHTML = `
                <button id="pr-audio-prev" style="background:none;border:none;color:#e5e7eb;cursor:pointer;font-size:14px;"></button>
                <button id="pr-audio-play" style="width:32px;height:32px;border-radius:999px;border:none;background:#22c55e;color:#022c22;font-weight:900;cursor:pointer;"></button>
                <button id="pr-audio-next" style="background:none;border:none;color:#e5e7eb;cursor:pointer;font-size:14px;"></button>
                <div style="min-width:180px;">
                    <div id="pr-audio-label" style="font-size:11px;text-transform:uppercase;letter-spacing:0.15em;color:#a5b4fc;">Step 1</div>
                    <div style="width:100%;height:4px;border-radius:999px;background:#1e293b;overflow:hidden;margin-top:4px;">
                        <div id="pr-audio-progress" style="height:100%;width:0%;background:#22c55e;"></div>
                    </div>
                </div>
            `;
            document.body.appendChild(bar);

            const playBtn = document.getElementById('pr-audio-play');
            const prevBtn = document.getElementById('pr-audio-prev');
            const nextBtn = document.getElementById('pr-audio-next');
            const label = document.getElementById('pr-audio-label');
            const progress = document.getElementById('pr-audio-progress');

            function updatePlayIcon() {
                playBtn.textContent = isUserPaused ? '' : '';
            }

            function updateLabel() {
                const step = steps[currentIndex];
                label.textContent = `${step.title || 'Step'}  ${currentIndex + 1}/${steps.length}`;
            }

            playBtn.addEventListener('click', function () {
                if (isUserPaused) {
                    playCurrent();
                } else {
                    pauseCurrent();
                }
            });
            prevBtn.addEventListener('click', prevTrack);
            nextBtn.addEventListener('click', nextTrack);

            audio.addEventListener('timeupdate', function () {
                if (!audio.duration) return;
                const pct = (audio.currentTime / audio.duration) * 100;
                progress.style.width = pct + '%';
            });

            audio.addEventListener('ended', function () {
                nextTrack();
            });

            // Initialize first step without autoplay
            loadTrack(0, false);
            updateLabel();
        })();
    </script>

</body></html>